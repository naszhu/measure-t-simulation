### 6.2.4 Results

The analysis presented in Lai, Cao, and Shiffrin (2024) encompassed a substantial dataset, combining the new experiments with the relevant conditions from the prior work of Nosofsky et al. (2021). This amounted to 376 distinct conditions for accuracy and another 376 for median correct response times across thirteen independent groups of participants. Before analysis, data from trials with response times that were exceptionally fast or slow were excluded to remove outliers. Specifically, for the new, more complex experiments, responses occurring before 180 ms or after 1,300 ms were removed (approximately 16% of the data), a decision made a priori to avoid biasing the subsequent analyses and modeling efforts (Lai, Cao, & Shiffrin, 2024).

The primary findings are organized around several key experimental comparisons, which collectively create a detailed empirical landscape that a comprehensive model of memory must navigate. The results for accuracy (probability of error) and median correct response times (RT) are presented as a function of set size and, more granularly, as a function of a target's serial position (lag) within a study list.

#### CM versus VM

A central objective of the research was to provide a clear empirical demonstration that the well-documented performance advantage of Consistent Mapping (CM) over Varied Mapping (VM) is attributable to response learning, separate from any potential confounds of item familiarity. As discussed in the Method section (6.2.3), the novel paradigm used in the present studies was specifically designed to equate the long-term familiarity of CM and VM items. The results confirmed a significant CM advantage. Across both pure and mixed conditions, performance was more accurate and faster for CM trials than for VM trials (Lai, Cao, & Shiffrin, 2024).

This superiority is particularly meaningful in the mixed conditions, where CM and VM items were intermingled within the same study lists, making it highly probable that participants applied the same general strategies to both item types. Because the new paradigm eliminated the familiarity confound, the observed CM advantage serves as direct evidence for the contribution of learning. Conservative sign tests on the mixed-condition data confirmed this conclusion, with the CM advantage being statistically significant for both accuracy (p=.004) and correct RT (p=.009) (Lai, Cao, & Shiffrin, 2024).

Interestingly, the magnitude of the CM superiority was noticeably smaller in the new studies compared to the very large advantage reported in Nosofsky et al. (2021). The authors attribute this difference to two factors. First, the prior study's standard CM paradigm allowed for a performance benefit from both learning _and_ extra familiarity for targets, whereas the new paradigm isolated the effect of learning alone. Second, the learning process itself may have been slower in the new, more complex paradigm, which required participants to learn a mapping between a picture and a specific response key, rather than simply learning to categorize an item as perpetually being a "target" or a "foil" (Lai, Cao, & Shiffrin, 2024).

#### Set-Size Effects

A robust and highly reliable finding across all conditions and experiments was a classic set-size effect. As the number of items in the study list increased (from 2 to 4 to 8/9), performance systematically declined. This was reflected in a higher probability of error (for both targets and foils) and longer response times for correct judgments. This pattern was clearly evident for all three main trial types—AN, VM, and CM—and was so strong as to be obvious by visual inspection of the data plots, obviating the need for statistical confirmation (Lai, Cao, & Shiffrin, 2024).

The authors highlight the particular theoretical significance of the strong set-size effect observed for _foils_. While the decline in performance for targets with increasing list length is also clear, much of that effect is explained by target recency, or lag, as will be discussed below. The fact that foil performance—which is independent of any specific study position—also degrades substantially with list length provides a critical constraint for modeling the decision process (Lai, Cao, & Shiffrin, 2024).

#### AN versus VM versus CM

When comparing performance across the three primary trial types in the pure conditions, a consistent hierarchy emerged: CM performance was the best, followed by All-New (AN), with VM performance being the worst. This rank ordering (CM > AN > VM) was large and reliable for both accuracy and RT. The same general pattern was observed in the mixed conditions, although the differences between the conditions were smaller, likely due to the increased noise resulting from fewer observations per data point (Lai, Cao, & Shiffrin, 2024).

The paper provides a clear theoretical interpretation for this hierarchy, which is central to the subsequent modeling effort. The superiority of CM over both AN and VM is attributed to the positive contribution of response learning, which is only possible in the CM condition. The superiority of AN over VM, meanwhile, is attributed to the differential effects of interference from long-term memory. In the AN condition, every test item is novel, meaning it has no matching event traces from prior trials. In the VM condition, however, a given test picture has been seen on many previous trials. These long-term event traces are non-diagnostic—they provide no useful information about whether the item is a target or a foil on the _current_ trial—and their activation during retrieval is posited to add variability, or noise, to the decision process, thereby harming performance relative to the "cleaner" AN trials (Lai, Cao, & Shiffrin, 2024).

#### Mixed/Pure Conditions

The performance advantage of CM over VM was consistently smaller in the mixed conditions than in the pure conditions. This moderation is explained by the reduced opportunity for learning in the mixed-list design. In a pure CM block, every trial reinforces the stimulus-response mappings. In a mixed block, CM items constitute only a fraction (e.g., one-half or one-third) of the studied items, occurring less frequently and thus slowing the rate of learning. Beyond this primary effect, the results from the mixed conditions were generally more variable, a natural consequence of having fewer trials contributing to each data point compared to the pure conditions (Lai, Cao, & Shiffrin, 2024).

#### Target Lag Effects

A powerful and universal finding in the data was a pronounced target lag effect, a form of recency. When performance for targets was plotted as a function of their position in the study list (where lag 1 is the most recently studied item), a steep gradient appeared. Targets presented more recently were associated with substantially higher accuracy and faster response times. This recency effect was a dominant feature of the data, appearing across all relevant conditions in every study (Lai, Cao, & Shiffrin, 2024). The authors interpret this finding as reflecting a fundamental property of short-term memory traces: their quality or accessibility decays with age, likely due to retroactive interference from subsequently presented items. The strength of this recency function provides another critical benchmark for evaluating the success of a computational model (Lai, Cao, & Shiffrin, 2024).

#### Target Lag Effects at Different Set Sizes

Perhaps one of the most theoretically intriguing results emerged from a finer-grained analysis of the interaction between target lag and set size. When the target lag functions for the different set sizes (e.g., lists of length 2, 4, and 8) were plotted on the same axes, they tended to lie almost directly on top of one another. This observation suggests that the large overall set-size effect for targets is primarily an artifact of averaging: longer lists necessarily have items at longer average lags, and it is this difference in lag, rather than list length _per se_, that drives the performance decline for targets (Lai, Cao, & Shiffrin, 2024).

However, this leads to an important "target-foil asymmetry." While performance for a target at a _given lag_ (e.g., lag 2) changes very little whether it comes from a list of 4 items or a list of 8 items, performance for _foils_ is strongly affected by list length, becoming worse as lists get longer. Since any robust measure of recognition memory (such as d') must take both target hits and foil false alarms into account, this asymmetry demonstrates that overall recognition ability does indeed decline for longer lists, even for targets at a fixed recency position. This nuanced pattern carries significant implications for theories of memory-based decision-making, particularly concerning how participants might set and adjust their response criteria in the face of varying list lengths. It implies that a single, fixed criterion might be used across different list lengths, which would naturally produce a large list-length effect for foils but not for targets at a specific lag (Lai, Cao, & Shiffrin, 2024).

In summary, the results presented in Lai, Cao, and Shiffrin (2024) established a complex but highly consistent set of empirical phenomena. The data confirm the roles of response learning, set-size effects, interference from prior-trial memory, and powerful recency effects. The specific patterns and asymmetries, particularly the CM-VM distinction in the new paradigm and the target-foil asymmetry in set-size effects, provide a rigorous and challenging testbed for the development and validation of the comprehensive computational model detailed in the subsequent sections of the paper.

