### **3.1 Rationale and Aims of Experiment 1**

Episodic memory, our ability to recollect specific past events, is fundamentally influenced by the context in which those events occurred. While the importance of context is widely acknowledged, the precise mechanisms by which it is encoded, represented, and utilized during memory retrieval remain active areas of investigation. As discussed in previous chapters, computational models of memory, such as the Retrieving Effectively from Memory (REM) framework, have provided invaluable tools for formalizing theories about these processes. However, a significant challenge persists in testing the core assumptions of these models, particularly concerning how memory operates over extended periods involving multiple, distinct episodes. Many standard recognition memory paradigms do not fully capture the dynamic interplay between item and context information, especially how context evolves across numerous learning events and how different types of contextual cues—some transient, some stable—contribute to recognition decisions. Experiment 1 was conceived to address these complexities directly through a robust, multi-list recognition paradigm designed to generate a rich, high-resolution dataset capable of constraining modern theories of memory.

The experiment's architecture—an initial phase of ten sequential study-test lists followed by a comprehensive final test—was deliberately chosen to simulate a more ecologically valid learning environment where information is accumulated over time. This structure allows for a systematic examination of memory phenomena at different timescales. Within each list, we can analyze local effects, such as how memory is affected by an item's position within a study or test sequence. Across the ten lists, we can track the global dynamics of memory, including the build-up of proactive interference and potential strategic adaptations by the learner. A core theoretical puzzle this design confronts is the surprising efficiency of memory. While adding hundreds of new items to memory across ten lists should theoretically produce crippling interference, human memory is often more resilient. This suggests the operation of a sophisticated mechanism for contextual filtering, which allows the cognitive system to effectively isolate memories from a target episode and ignore a vast repository of irrelevant information. This experiment seeks to characterize the behavioral markers of such a mechanism.

A central and novel feature of the initial phase design is the specific composition of each list: participants study 20 pictures but are then tested on only 10 of those pictures (targets) intermixed with 10 entirely new pictures (novel foils). This creates two distinct classes of items that have been studied within each list: those that are subsequently tested (Studied-and-Tested, or S&T) and those that are not (Studied-Only, or SO). This manipulation is not incidental; it is a core element of the experiment's rationale, designed to isolate the memorial consequences of the act of testing itself. The S&T items provide an opportunity for their initial memory traces to be strengthened, modified, or updated via a successful retrieval event, a phenomenon related to the well-known "testing effect." In contrast, the SO items serve as a crucial control group, representing a "purer" form of encoding without the potentially confounding influence of an initial test. By comparing subsequent memory for these two item types in the final test phase, we can directly and quantitatively assess the impact of interim testing on the long-term durability and accessibility of episodic memories.

Furthermore, the exclusive use of novel foils during these initial ten lists is a critical methodological control. By ensuring that the "New" items in each test are truly new to the entire experiment, we simplify the recognition judgment. The task for the participant is to distinguish a recently studied item from an item never seen before, rather than making a more complex source memory judgment (e.g., "Was this item on this list or a previous list?"). This provides a cleaner measure of proactive interference, as any performance decline across lists can be more confidently attributed to the accumulating memory traces of prior _targets_, not ambiguity arising from re-presented foils. This establishes a clear and interpretable baseline of performance, setting a foundation for Experiment 2, where the nature of foils will be explicitly manipulated.

The second major component of the experiment is the surprise final test phase, which serves as the ultimate arbiter of what has been retained. In this phase, all 200 items originally presented during the study phases are re-presented for a final recognition judgment. This allows for a comprehensive assessment of long-term memory that leverages the key manipulations from the initial phase. First, as noted, it enables the critical comparison between S&T and SO items. Second, it allows for a full analysis of long-term primacy and recency effects across the entire 10-list session. Are items from the very first list still accessible, and if so, how does their memory strength compare to items from the final list? The answers provide deep insights into the persistence of contextual representations over time.

To further probe the nature of these long-term contextual representations, the order of item presentation during this final test is systematically manipulated. Participants are assigned to one of three conditions: a _Forward_ condition that preserves the original presentation order of the lists, a _Backward_ condition that reverses this order, and a _Random_ condition that serves as an unstructured baseline. This manipulation is designed to test specific hypotheses about context reinstatement. For example, if the memory system encodes a gradually changing temporal context, the Forward condition should most effectively reinstate this context, potentially facilitating retrieval and boosting performance, especially for items from early lists. The Backward condition, conversely, disrupts this forward-flowing temporal context, which may produce a different signature of memory performance. The patterns of results across these three conditions can therefore reveal crucial information about the structure of stored contextual information and how retrieval cues interact with that structure.

Finally, a primary and overarching objective of Experiment 1 is to generate a rich empirical dataset specifically tailored for the purpose of developing, constraining, and rigorously testing the extended REM-based computational model presented in Chapter 5. The detailed behavioral patterns—from within-list serial position curves, to between-list interference effects, to the differential fate of S&T and SO items, to the distinct performance profiles in the three final test conditions—serve as critical benchmarks. A successful model must not merely capture an aggregate measure of performance but must be able to replicate this entire constellation of findings. This experiment is therefore a direct bridge between empirical investigation and computational theory, designed to provide the necessary data to validate and refine the model's core mechanisms for context representation, context drift, memory updating, and context-based filtering in recognition decisions.

By pursuing this multi-faceted approach, Experiment 1 aims to achieve the following specific objectives:

1. **Examine within- and between-list contextual effects:** To investigate how recognition accuracy is influenced by an item's study and test position within a list, and to track how performance (hits, correct rejections) evolves across the sequence of ten lists as proactive interference accumulates.
2. **Isolate the memorial consequences of testing:** To directly compare long-term recognition for items that were only studied (SO) versus those that were studied and then immediately tested (S&T), thereby quantifying the impact of the initial retrieval event on trace strength and accessibility.
3. **Investigate the influence of retrieval context structure:** To use a final, cumulative test with forward, backward, and random presentation orders to probe how reinstating or disrupting the original temporal context affects long-term memory retrieval.
4. **Provide stringent empirical benchmarks for computational modeling:** To generate a detailed dataset that can be used to validate and constrain an extended REM-based model, ensuring its mechanisms can account for the complex patterns of context-dependent memory observed across multiple timescales and learning histories.


### **3.2. Methodology**

This section provides a comprehensive and highly detailed description of the methods, materials, and procedures employed in Experiment 1. The experimental design was implemented as a web-based study with precise, code-level control over all parameters to ensure the highest possible data quality and procedural standardization. Every aspect of the methodology, from initial participant screening and the technical environment to the specific timing of stimulus presentation and the nature of multi-layered feedback systems, was deliberately constructed to address the research aims outlined in Section 3.1. The goal was to create a robust and engaging experimental paradigm capable of generating a rich, high-fidelity dataset for investigating the dynamic role of context in recognition memory and for the rigorous development and validation of computational models.

#### **3.2.1. Participants and Recruitment Environment**

Participants were recruited online via the Prolific academic research platform (prolific.com), which facilitates access to a large and more demographically diverse sample compared to traditional laboratory-based recruitment. An initial target sample size was determined based on a power analysis conducted prior to data collection, aiming to ensure sufficient statistical power (β > 0.80) to detect medium-sized effects for the key a-priori comparisons of interest, such as performance changes across the ten pretest lists and memory differences between items with varying learning histories in the final test.

To ensure a high standard of data quality and participant engagement, several layers of screening and eligibility requirements were enforced. Participation was restricted to individuals residing in the United States, who were at least 18 years of age, reported fluency in English, and maintained a prior submission approval rating of at least 98% on the Prolific platform. This high approval rating serves as a reliable heuristic for participant conscientiousness and data integrity. Before beginning the experiment, individuals were required to enter their unique Prolific ID, which was then cross-referenced against a master list of IDs from individuals who had previously participated in this or related studies; any matching IDs were immediately excluded to prevent non-naive participation and ensure data independence.

A crucial component of the recruitment and consent process was the explicit articulation of a performance-contingent compensation model. The detailed information sheet presented at the outset clearly stated that the compensation rate was $10.50 per hour, but that payment was contingent upon successful completion of the experiment, defined as achieving an overall accuracy of at least 60%. This threshold, well above the 50% chance level, was designed to be easily achievable by any participant who remained attentive and engaged, while simultaneously serving as a robust mechanism to disincentivize low-effort or random responding. This motivational framework is a critical methodological tool for ensuring valid data collection in lengthy and cognitively demanding online experiments.

The entire experimental protocol, including the recruitment strategy, consent process, and compensation structure, received full ethical review and approval from the [Placeholder: e.g., "Institutional Review Board (IRB) of Indiana University, under protocol 18431"]. All participants were required to read the detailed information sheet and provide active digital consent before any experimental procedures could be initiated.

#### **3.2.2. Materials, Apparatus, and Software**

**Stimuli** The stimuli consisted of a master pool of 2,361 unique, high-quality color pictures sourced from various image libraries. This large pool ensured that every image presented to a participant as a study item or a foil (both during the pretest and final test phases) was unique within that participant's session, preventing any confounding effects of item repetition across different experimental roles. The specific images for each participant's session were drawn systematically from a single, globally shuffled master list (`picnames_random`) to ensure randomization while maintaining strict control over stimulus allocation, as detailed in the Design section below.

**Experimental Software and Environment** The experiment was implemented using the jsPsych library, a JavaScript-based framework designed for creating and running behavioral experiments with precise timing in a web browser. The use of jsPsych ensured millisecond-level accuracy for all stimulus presentation durations, inter-stimulus intervals (ISIs), and the recording of response times. The experiment was hosted on a dedicated server optimized for psychological research, ensuring reliable performance.

To create a standardized and controlled testing environment, several technical constraints were enforced programmatically. A browser-check function at the start of the script verified that participants were using a desktop computer and the Google Chrome web browser; any participant failing this check was denied access and instructed to switch devices or browsers. This step was crucial for ensuring consistency in how stimuli were rendered and for avoiding the known variability and distractions associated with mobile devices. To guarantee a smooth and uninterrupted experimental flow, a preloading function was executed before the first trial, which downloaded all required image files to the participant's local browser cache. This prevented trial-to-trial lag or stuttering that could result from loading images on the fly. Finally, immediately after providing consent, participants were required to switch their browser to full-screen mode, which was enforced by the script. This minimized potential visual distractions from other applications, browser tabs, or desktop notifications, further enhancing the laboratory-like control of the online environment. To prevent data loss from prolonged inactivity, the experiment was programmed with a 120-minute inactivity timeout, after which the session would automatically terminate.

#### **3.2.3. Design and Procedure**

The experiment employed a complex, multi-phase, within-subjects design that guided each participant through a structured sequence of learning and testing. The procedure was designed to first build up a rich, multi-layered memory history over an extended pretest phase, and then to comprehensively probe the resulting memory representations in a final test phase. The procedure unfolded chronologically as follows: (1) a Practice Block, (2) ten sequential Main Blocks, and (3) a final, cumulative Recognition Test. A between-subjects manipulation was applied to the ordering of the final test.

**Timing Parameters** All trial event durations were precisely controlled. Key timing parameters for the main experimental phases were as follows:

- **Study Image Presentation:** 2000 ms
- **Inter-Stimulus Interval (ISI):** 100 ms (typically a blank screen)
- **Fixation Cross Duration:** 1000 ms
- **Distractor Digit Presentation:** 2000 ms per digit, with a 1000 ms gap
- **Distractor Feedback:** 2000 ms
- **Pretest Recognition Response Window:** 3500 ms
- **Final Test Response Window:** 4000 ms
- **Response Time Warning Cutoff (Too Fast):** 150 ms
- **Warning Message Display Duration:** 1500 ms
- **Final Test Provenance Feedback:** 500 ms

**Stimulus Allocation** The allocation of the 2,361 images to specific roles was performed systematically. After a one-time shuffle of the master list, the first 200 images were designated as the study items for the ten pretest lists (20 per list). The next 100 images were designated as the foils for the ten pretest lists (10 per list). The subsequent 210 images were reserved to serve as novel foils exclusively for the final recognition test. This sequential assignment from a single randomized list ensured that there was no overlap in the pools of images used for study, pretest foils, and final test foils.

**Practice Block (Block 1)** The first block served as a mandatory, hands-on tutorial. Participants were presented with detailed instructions explaining the three-part structure of a block: study, digit-sum distractor, and recognition test. They were informed of the response keys ('J' for old, 'F' for new) and the performance contingency (≥60% accuracy). The block then proceeded with the standard structure: study 20 images, complete the distractor task, and perform a 20-trial recognition test. This practice phase was critical for ensuring participants fully understood the tasks, the timing, and the two distinct feedback systems (arithmetic feedback and running recognition accuracy) before beginning the formal data collection blocks.

**Main Blocks (Blocks 2–10): The Pretest Phase** After receiving a brief message indicating the end of the practice, participants proceeded through ten sequential main blocks. Each block followed the same tripartite structure, designed to create a distinct episodic learning event.

- **Study Phase:** Each block began with a 1-second fixation cross, followed by the presentation of 20 unique study images in a randomized order. Each image was displayed for 2000 ms, separated by a 100 ms ISI.
    
- **Distractor Phase:** This phase was designed to prevent active rehearsal of the studied images. After a 3-second prompt ("Summing up the digits..."), participants saw eight random digits (sampled from 4–9) presented sequentially, each for 2000 ms with a 1000 ms gap. Afterward, they were prompted to type the sum. The script checked for a valid numeric input and provided immediate feedback: "CORRECT!" or "INCORRECT!" for 2000 ms.

%% needs to emphysis feedback kind as a seprate single thing, i  think  %%
- **Recognition Phase:** Following the distractor, a 20-trial recognition test began. The test list was a randomized sequence of 10 "target" items (randomly selected from the 20 just-studied images) and 10 "foil" items (novel images drawn from the pretest foil pool). For each trial, an image was shown for up to 3500 ms, and the participant responded 'J' (old) or 'F' (new). To ensure data quality, responses faster than 150 ms triggered a "Too fast!" warning, while a failure to respond within the 3.5-second window triggered a "You need to respond faster!" warning. A key feature of this phase was the continuous feedback on recognition performance Underneath the response key reminder, a line of text read, "Your accumulated accuracy for this trial is: XX%," which was updated after each response within that block. This feedback system was designed to maintain participant motivation and engagement across the ten blocks.
    

**Final Recognition Test** After the tenth main block, the final, surprise test phase was initiated. The instructions for this phase were critical: participants were explicitly told to respond "J" (old) if they had seen a picture _at any time during the experiment_, including as a studied item or as a foil, and "F" (new) only if it was brand new.

- **Stimulus Composition:** This test consisted of a single, long block of 420 trials, comprising 210 "old" items and 210 novel foils. The composition of the "old" items was a core element of the experimental design, providing the statistical power to compare memory for items with different learning histories. For each of the ten pretest blocks, the script randomly sampled 7 items from each of three distinct categories:
    
    1. **Studied-and-Tested (S&T):** 7 of the 10 items that were studied and subsequently appeared as targets in the pretest recognition phase.
    2. **Studied-Only (SO):** 7 of the 10 items that were studied but were _not_ selected to be targets in the pretest recognition phase.
    3. **Previously-Tested Foils (PTF):** 7 of the 10 items that appeared as foils during the pretest recognition phase. This sampling resulted in 210 "old" trials (10 lists × (7 S&T + 7 SO + 7 PTF)), which were then intermixed with 210 brand-new foils. This design allows for a powerful analysis comparing recognition of items that were deeply encoded and retrieved (S&T), items that were only encoded (SO), and items that were merely exposed and rejected (PTF).
- **Procedure and Feedback:** Each of the 420 trials presented an image for up to 4000 ms. The running accuracy feedback was absent in this phase. Instead, after each response, a form of _provenance feedback_ was provided. The same test image was re-displayed for 500 ms with an overlaid message indicating its origin, such as, "This item had been studied or tested on list 7" or "This item had never been seen previously". This non-performative feedback was designed to maintain engagement during the long final test by providing informative, trial-relevant details. The same warnings for too-fast or too-slow responses remained active.
    
- **Order Manipulation:** A between-subjects factor controlled the presentation order of the 420 final test trials. Participants were randomly assigned to one of three conditions (`condi`): **Forward**, where trials were blocked and presented in the original list order (Block 1 items, then Block 2, etc.); **Backward**, where the order was reversed (Block 10 items first); or **Random**, where all 420 trials were uniformly shuffled.
    

**Completion and Debriefing** Upon completion of the final trial, full-screen mode was automatically exited. A final summary screen displayed the participant's overall accuracy for the final test and reiterated that a completion code would be provided if the 60% accuracy threshold was met. After the participant pressed a key, the script saved all data to a local CSV file on the participant's machine (`ekstra.csv`) and automatically redirected them to the Prolific completion URL to claim their compensation.

#### **3.2.4. Data Recording and Key Dependent Measures**

The jsPsych script was designed to save a comprehensive, trial-by-trial dataset. For every event, key variables were logged, enabling a multi-layered analysis of performance. Key dependent measures included:

- **Pretest Performance:**
    
    - **Distractor Accuracy:** The correctness of the sum response for each of the ten arithmetic tasks (`correct`). This serves as a measure of continued engagement in the distractor task.
    - **Recognition Accuracy:** The proportion of correct "old" and "new" judgments (`recognition_correct`) within each of the ten pretest lists. This can be broken down into Hit Rates for `TARGET_target` items and Correct Rejection rates for `TARGET_foil` items.
    - **Response Times (RTs):** The latency of recognition judgments, allowing for analyses of decision speed.
    - **Within-List Effects:** By using the logged study position (`prespos`) and test position (`testpos`), it is possible to analyze serial position curves and output interference effects within each list.
- **Final Test Performance:**
    
    - **Recognition Accuracy:** The overall correctness (`recognition_correct`) across all 420 trials, broken down by item type. The primary analysis will involve comparing the Hit Rates for the three critical "old" categories: S&T, SO, and PTF. The False Alarm rate to the 210 novel foils will also be calculated.
    - **Effect of Retrieval Order:** Comparing accuracy and RT patterns across the Forward, Backward, and Random conditions will reveal the influence of retrieval structure on long-term memory access.
    - **Long-Term Serial Position Effects:** Using the logged original list number (`prespos_itrial`) and study position (`prespos_iposintrial_study`), it is possible to reconstruct the original learning sequence and analyze long-term primacy and recency effects.
- **Compliance Measures:** The number of "Too Fast" and "Too Slow" warnings triggered by each participant provides a metric of attention and compliance with task instructions.