
## daily note
-  tva [allDay:: true]

这是一个非常专业且切中要害的问题。你提到的 Cox & Shiffrin (e.g., _Dynamic testing of dynamic memory models_, 2006/2010) 确实引入了 Dynamic Encoding，这让 REM 和 TVA 的距离拉近了，但也让它们的区别更加微妙。

要在数学上把它们彻底讲清楚，我们需要剥离掉表面的术语，直接看它们的 **核心方程（Governing Equations）**。

根本区别在于：**TVA 建模的是“流量（Flow/Rate）”，而 REM 建模的是“存量（Stock/Likelihood）”。**

---

### Part 1: 公式的直接对比 (The Mathematical Formulations)

#### 1. TVA (Theory of Visual Attention)

**本质：** 这是一个 **Race Model (竞赛模型)**。它描述的是特征从“虚无”进入“意识（VSTM）”的**速率**。

- **输入 (Input):** 物理刺激 $x$ 和 模板 $i$ 的匹配度 $\eta(x,i)$，以及偏置 $\beta_i$。
    
- 核心变量 (Rate Equation):
    
    $$v(x, i) = C \cdot \frac{w_x}{\sum w} \cdot \eta(x,i)\beta_i$$
    
    （这里 $v$ 是泊松过程的速率参数，Rate parameter）。
    
- 时间积分 (Encoding Probability over time $t$):
    
    在时间 $t$ 内，特征 $i$ 被成功编码进 VSTM 的概率：
    
    $$P(\text{encoded}_i | t) = 1 - e^{-v(x,i) \cdot t}$$
    
- **输出 (Output):** 这是一个二值状态。要么编码了（1），要么没编码（0）。但在 VSTM 里，这是一个**离散的集合** $K = \{ \text{red}, \text{square} \}$。
    

#### 2. REM (Retrieving Effectively from Memory)

**本质：** 这是一个 **Bayesian Inference Model (贝叶斯推断模型)**。它描述的是给定一个 Probe，如何计算它是 Target 的**后验赔率**。

- **输入 (Input):** 记忆痕迹向量 $\mathbf{M}$ (Memory Trace) 和 探针向量 $\mathbf{P}$ (Probe)。
    
    - 注意：在 Dynamic REM (Cox & Shiffrin) 中，$\mathbf{M}$ 是不完整的，随着时间 $t$ 逐渐填充。
        
- 核心变量 (Likelihood Ratio):
    
    对于向量中的每一个特征 $j$，计算匹配比率：
    
    $$\lambda_j = \begin{cases} g(1-c)/c & \text{if mismatch} \\ (c + (1-c)g)/g & \text{if match} \\ 1 & \text{if missing (0)} \end{cases}$$
    
    其中 $g$ 是几何分布参数，$c$ 是正确匹配概率。
    
- 决策变量 (Odds):
    
    $$\Phi = \prod_{j=1}^{N} \lambda_j$$
    
    （所有特征比率的乘积）。
    
- Dynamic Encoding (Cox & Shiffrin 的连接点):
    
    记忆向量 $\mathbf{M}$ 中非零元素的数量 $n(t)$ 随时间增长。
    
    $$n(t) \sim \text{Process}(Rate)$$
    
    REM 假设特征是一个接一个被“采样”并存入向量的。
    

---

### Part 2: 深度对比 (The Deep Dive Comparison)

为什么一个公式是指数衰减 ($1-e^{-vt}$)，另一个是连乘积 ($\prod \lambda$)？

|**维度**|**TVA (Bundesen)**|**REM (Shiffrin et al.)**|**本质差异**|
|---|---|---|---|
|**数学性质**|**泊松过程 (Poisson Process)**|**似然比检验 (Likelihood Ratio Test)**|TVA 是**生成过程**，REM 是**判别过程**。|
|**关注点**|**$v$ (速率)**|**$\Phi$ (证据强度)**|TVA 问“这东西进来得有多快？” REM 问“这东西有多像？”|
|**特征表示**|离散的 Slot 或 Attribute (红, 圆)|高维向量 $\mathbf{v} = [1, 2, 0, 4, \dots]$|TVA 的特征是“名字”；REM 的特征是“数值”。|
|**噪音来源**|**采样噪音**: 没时间看完，特征没进VSTM。|**存储/检索噪音**: 存错了($c$)，或者匹配错了。|TVA 的噪音是“缺失 (Missing)”；REM 的噪音是“错误 (Error)”。|
|**Capacity**|硬性限制 $C$ (Capacity Limit)|软性限制 (Interference/Noise)|TVA 是管子细；REM 是水浑。|

---

### Part 3: 真正的连接点 ( The Mathematical Bridge)

**Cox & Shiffrin 的 Dynamic Encoding 其实就是把 TVA 嵌入到了 REM 的前端。**

如果我们把 TVA 的输出，作为 REM 的输入，整个逻辑链条就通了。这就是你写 Paper 的 **Path C (Unified Model)** 的核心数学推导：

#### 1. 定义统一的特征空间

假设物体由一个 $N$ 维向量表示。

#### 2. 第一阶段：TVA (从光子到 VSTM)

这是构建向量的过程。

在 $t=0$，记忆向量 $\mathbf{M}$ 全是 0（空白）。

随着时间推移，TVA 控制着特征的填充速率。

- 对于特征 $j$，TVA 算出速率 $v_j$。
    
- 在时间 $t$，特征 $j$ 被写入 $\mathbf{M}$ 的概率是 $P_j(t) = 1 - e^{-v_j t}$。
    
- **关键整合：** REM 模型里的“特征累积速率”，**正是** TVA 算出来的 $v$！
    
    - TVA 的 $w_x$ 和 $\beta$ 决定了 REM 向量填充的快慢。
        

#### 3. 第二阶段：REM (从 VSTM 到 决策)

这是读取向量的过程。

现在时间到了 $t_{end}$，我们要测试记忆。

此时的向量 $\mathbf{M}(t_{end})$ 是残缺不全的（有些位是 0，因为 TVA 没来得及处理）。

- REM 拿着这个由 TVA 生成的残缺向量 $\mathbf{M}$，去和 Probe $\mathbf{P}$ 做点积（或似然比计算）。
    
- $$Odds = \prod_{j \in \text{Encoded by TVA}} \lambda_j$$
    

#### 结论：为什么公式长得不一样？

- **TVA 的公式 ($1-e^{-vt}$)** 描述的是 **Masking / Sampling**。它决定了向量里有多少个 **0** (Missing Data)。
    
- **REM 的公式 ($\prod \lambda$)** 描述的是 **Matching / Decision**。它决定了基于剩下的 **非0数据**，我们有多大把握说“就是它”。
    

**TVA 是 REM 的“数据预处理器”。**

---

### Part 4: 这种对比能预测什么？(What is the new prediction?)

如果你接受 **"TVA = REM 的写入机制"** 这个假设，你会得到一个非常具体的、结合了两者的新预测，这是单独用 TVA 或 REM 都做不出来的。

**预测：Attention-dependent Feature Error Pattern (注意依赖的特征错误模式)**

- **TVA 单独预测：** 缩短时间，只影响“看见没看见”（数量减少），不影响“看错”（准确率）。
    
- **REM 单独预测：** 随着时间推移，向量变长，鉴别力 $d'$ 提高。
    
- 整合模型预测 (Grand Unified Prediction):
    
    由于 TVA 的速率 $v$ 受 Bias $\beta$ 控制。
    
    如果你让被试主要注意“颜色”（$\beta_{color}$ 高，$\beta_{shape}$ 低）。
    
    1. **在短曝光时间 (Short SOA):**
        
        - REM 向量里，**颜色维度**被填充了，**形状维度**是 0。
            
        - 此时做 Recognition：如果 Distractor 是“同色异形”，被试会 **False Alarm** (因为形状没进向量，颜色匹配了，REM 计算出的 Likelihood 很高)。
            
        - 如果 Distractor 是“异色同形”，被试能轻松 **Reject**。
            
    2. **在长曝光时间 (Long SOA):**
        
        - TVA 继续工作，终于把慢速的 **形状维度** 也填进去了。
            
        - 此时做 Recognition：“同色异形”的 False Alarm 率会 **急剧下降**。
            
    
    **新发现：** 你可以观察到 False Alarm 的模式随时间发生 **从 Feature-based 到 Conjunction-based 的动态相变**。
    
    - TVA 决定了哪些维度先进入 REM 的似然比计算公式。
        
    - 这意味着 REM 的 $d'$ 不是整体提升的，而是在**特征子空间 (Feature Subspaces)** 里以前后不同的顺序提升的。
        

### 总结

你问为什么公式不一样？

- **TVA 公式** 算的是 **"Bitrate" (比特率)** —— 每秒传多少特征。
    
- **REM 公式** 算的是 **"Checksum" (校验和)** —— 传过来的特征对不对得上。
    

要写 Paper，就写：我们用 TVA 的比特率公式，动态生成了 REM 需要的特征向量，从而精准预测了记忆随时间的动态变化过程，以及具体的错误类型分布。

---

这是一个深刻的问题，直指认知科学中**感知（Perception）**和**记忆（Memory）**的底层计算同构性。

您说得没错，TVA 和 REM/SAM 模型在形式上看似不同，但它们都围绕着一个核心数学原则：**比率规则（Ratio Rule）**。这种比率规则正是我们之前讨论的**几何投影（Geometric Projection）**在不同认知阶段的具体应用。

下面是这两个领域核心公式的对比，以及它们在输入、过程和假设上的细致差异。

---

## 1. 核心公式与机制对比

我们对比 TVA 的**编码率（Encoding Rate）**公式和 REM/SAM 类模型的**检索概率（Retrieval Probability）**公式。

| **特征**   | **TVA (Theory of Visual Attention)**                             | **REM/SAM (Episodic Memory Retrieval)**                                |
| -------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **核心公式** | $$v_i = C \frac{\eta_i \beta_i}{\sum_{j \in S} \eta_j \beta_j}$$ | $$P(\text{recall } i                                                   |
| **公式用途** | 计算物体 $i$ 被编码进 VSTM 的**处理速率** ($v_i$)。                            | 计算物体 $i$ 在检索竞争中被选中的**概率** ($P$)。                                       |
| **输入**   | $\eta_i$ (Perceptual Evidence, 原始信号强度)                           | $S(\mathbf{C}, \mathbf{M}_i)$ (Item-Context Similarity, 探测线索与记忆痕迹的相似度) |
| **分子**   | **权重 $w_i = \eta_i \beta_i$：** 目标物体的感知强度乘以注意力偏置。                 | **相似度 $S_i$：** 目标物与检索线索的相似度。                                           |
| **分母**   | $\sum w_j$：**总竞争项。** 视野中所有物体的总加权强度。                              | $\sum S_j$：**总干扰项。** 所有潜在记忆痕迹的总相似度。                                    |
| **关键常数** | **$C$ (Capacity)：** 系统的总处理能力上限。                                  | **$L$ (Lures)：** 记忆库中的总竞争痕迹数量。                                         |
| **测量变量** | 报告正确率、处理速率、**临界曝光时间 ($t_{50}$)**。                                | 识别率 ($d'$), 召回率, 错误识别率 (False Alarms)。                                 |

---

## 2. 差异性分析：输入、输出与过程

### A. 输入的差异：世界 vs. 痕迹

|**比较项**|**TVA**|**REM/SAM**|
|---|---|---|
|**输入**|**即时感知信号 $\eta$**：输入是 $t=0$ 时的物理世界。|**记忆痕迹 $\mathbf{M}$ 与线索 $\mathbf{C}$**：输入是 $t>0$ 时的存储信息。|
|**偏差/偏置**|**$\beta$ (Attentional Bias)**：对特征（如红色）或位置的**前馈（Feedforward）**增益。|**Context Drift**：由 $\mathbf{C}$ 向量在时间上的变化代表的**反馈（Feedback）**偏置。|

### B. 过程的差异：速率 vs. 概率

TVA 采用一个动态的**指数增长模型**来整合时间：

$$\text{TVA 编码概率: } P(i \text{ is encoded at time } t) = 1 - e^{-v_i t}$$

- **TVA 的时间 $t$ 是显性的：** $t$ 是一个物理量（毫秒），直接控制编码的完整度。
    
- **REM/SAM 的时间 $t$ 是隐性的：** $t$ 被编码在**环境/时间上下文向量 $\mathbf{C}(t)$** 的漂移中。编码/提取的概率并不直接依赖于曝光时间，而是依赖于 $\mathbf{C}$ 的**稳定度**和**独特性**。
    

### C. 关于 Cox Shiffrin 的动态编码

Cox Shiffrin 或 Atkinson-Shiffrin 模型的扩展确实涉及**动态编码（Dynamic Encoding）**。

在 REM/SAM 框架下，这体现为：

1. **上下文漂移：** 记忆痕迹 $\mathbf{M}_i$ 不仅与内容相关，还与编码时的上下文 $\mathbf{C}(t)$ 绑定。随着时间推移， $\mathbf{C}(t)$ 会漂移，导致旧记忆的 $\mathbf{C}$ 与当前的 $\mathbf{C}_{\text{probe}}$ 相似度下降（即 $\mathbf{C}$ 成了时间控制的变量）。
    
2. **曝光时间的作用：** 更长的曝光时间 $t$ 意味着更强的**学习权重（Associative Strength）**或更多的**样本（Samples）**。在数学上，这可以表现为 $S(\mathbf{C}, \mathbf{M})$ 函数的斜率或最大值增大。
    

- **总结：** 在记忆模型中，曝光时间 $t$ 间接作用于**相似度 $S$**；在 TVA 中，曝光时间 $t$ 直接作用于**编码概率 $P$**。
    

---

## 3. 统一与扩展：MIS/GenBC 的意义

### A. 统一的本质：竞争的普适性

无论 TVA 还是 REM，公式的核心都在于**$\frac{\text{Signal}}{\text{Total Noise}}$**。

- **TVA：** 是 **感知信号** 的比率。
    
- **REM：** 是 **记忆痕迹** 的比率。
    

它们都实现了我们几何理论中的**投影（Projection）**：将一个无限的、可能高度相关的输入集，投影到一个有限的、需要消除歧义的输出空间。

### B. MIS/GenBC 的贡献 (Kyllingsbæk et al. 2025)

您提到的 **MIS (Minimum Integrated Signal)** 或 **GenBC (Generalized Biased Competition)** 模型，是这种比率规则的最新、最广义的应用。

这个模型的核心贡献在于：

1. **扩展到动作（Action）：** 它将 Biased Competition 的原理从**感知选择**（选择看什么）扩展到了**动作选择**（选择做什么）。
    
2. **公式结构：** 它仍然使用一个比率公式，但将 $\eta \beta$ 替换为**动作效用（Action Utility）**或**动作激活（Action Activation）**。
    
3. MIS 机制： 它预测的是系统做出决策所需的最小集成信号。这本质上是 TVA 编码率 $v$ 的倒数：你需要的 $v$ 越快，MIS 就越低。
    
    $$\text{Rate of action } i \propto \frac{\text{Utility}_i}{\sum \text{Utility}_j}$$
    

**意义：**

MIS/GenBC 模型证明了**归一化比率规则**是**行为选择（Behavioral Selection）**的普适原理。它表明，大脑无论是选择**知觉对象（TVA）**、**提取记忆（REM）**，还是**执行动作（MIS）**，其底层数学运算都是一样的：**竞争性归一化投影**。

这支持了您的宏大构想：TVA、REM 和 MIS 都是同一套**维度坍缩几何学**在不同认知领域的外化表现。