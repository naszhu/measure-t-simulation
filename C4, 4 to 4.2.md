


## **Chapter 4: Experiment 2 - Picture Recognition with Mixed Foils** 

## **Chapter 4: Experiment 2 - Picture Recognition with Mixed Foils**

### **4.1. Rationale and Aims of Experiment 2**

Experiment 1 provided foundational insights into the role of context in picture recognition, particularly examining within-list and between-list effects using exclusively novel foils during the initial test phases. This initial approach, while methodologically clean, established a baseline for recognition performance under conditions where lures lacked any prior episodic history within the experimental session. However, memory in everyday life seldom operates in such a pristine environment. We are constantly faced with the challenge of distinguishing currently relevant information from a vast store of similar or related past experiences. For instance, recognizing a person in a new setting requires differentiating them not only from complete strangers but also from individuals who might share perceptual features or who were encountered in different, perhaps irrelevant, past contexts. The exclusive use of novel foils in Experiment 1, therefore, may not have fully captured the nuanced cognitive processes involved in resolving interference from stimuli that possess varying degrees of familiarity or prior contextual associations. Specifically, it may not have maximally taxed the memory system's reliance on precise contextual information to disambiguate targets from lures that share specific episodic histories or perceptual features with studied items. The current experiment, Experiment 2, was conceived to address this limitation by introducing a more ecologically representative and mnemonically challenging recognition environment.

The primary advancement of Experiment 2 over its predecessor lies in the strategic incorporation of mixed foil types during the initial, list-by-list test phases. These foils were not limited to entirely new pictures but also included pictures that participants had encountered previously within the experiment, albeit in different roles or lists. This included items that were studied in prior lists but not tested, items that were tested as targets in prior lists, or even items that had previously served as novel foils. Such a manipulation is critical because different types of prior exposure are known to generate distinct forms of mnemonic interference. For example, items previously studied but not tested might elicit a general sense of familiarity without a strong, specific contextual tag from a recent test event, potentially leading to source monitoring errors. Items previously tested as targets might carry strong traces from both study and test, making them highly familiar and thus potent lures if their original list context cannot be precisely retrieved. Even items previously encountered as foils, if sufficiently processed, might accrue some familiarity, contributing to the overall interference landscape. The introduction of these "prior-exposure" foils aimed to create higher levels of proactive and retroactive interference, thereby compelling participants to engage more deeply with, and rely more heavily upon, the specific contextual details associated with each studied item to make accurate recognition judgments. By systematically varying the history of foil items, Experiment 2 seeks to dissect how different forms of prior experience influence the perceived familiarity of a test probe and the subsequent decision processes. This approach allows for a more granular investigation of how context is encoded, updated, and utilized to navigate a complex memory environment where simple familiarity is an unreliable guide.

[familarity here? idk]The theoretical impetus for this design stems from several key concepts in memory research. Firstly, the distinction between familiarity and recollection (e.g., Yonelinas, 2002) is central. While novel foils in Experiment 1 might be rejected based on a lack of familiarity, mixed foils, particularly those with prior study or test exposure, are likely to elicit varying degrees of familiarity. Correctly rejecting such foils would necessitate a greater reliance on recollection—the retrieval of specific qualitative information about a prior encounter, including its context. Secondly, theories of interference, such as the Retrieving Effectively from Memory (REM) model (Shiffrin & Steyvers, 1997) which forms the basis for the computational work in this dissertation, posit that memory retrieval is an error-prone process where the match between a probe and stored memory traces is assessed. Traces from different episodes can all contribute to the evidence for a probe, and the model explicitly incorporates mechanisms for how context influences trace activation and evaluation. Experiment 2 provides a rich testbed for these mechanisms, as the varied foil histories should differentially affect the likelihood of spurious matches and the utility of contextual filtering. Thirdly, the concept of source monitoring (Johnson, Hashtroudi, & Lindsay, 1993) is highly relevant. When faced with a familiar-seeming foil, participants must engage in source monitoring to determine if its familiarity stems from the current study list or from a different, irrelevant prior episode. The design of Experiment 2, with its carefully controlled item histories, allows for a systematic exploration of these source attribution processes.

Building upon this rationale, Experiment 2 was designed with the following specific aims:

1. **Investigate the impact of varied foil types on recognition performance:** The primary objective is to systematically examine how hit rates for targets and, crucially, false alarm rates for different categories of foils (entirely novel foils, foils previously studied but not tested in a prior list, foils previously tested as targets in a prior list, and foils previously encountered as novel foils in a prior list) evolve both within each study-test list and across the sequence of ten lists. We anticipate that false alarm rates will be lowest for entirely novel foils and progressively higher for foils with richer prior episodic histories, reflecting increased interference. Analyzing these patterns will illuminate the types of contextual information (or its absence) that lead to memory errors versus successful discriminations, and how the memory system adapts (or fails to adapt) to these different levels of lure confusability over time.
    
2. **Compare recognition dynamics with Experiment 1:** A direct comparison of performance metrics (accuracy, response times, and potentially signal detection parameters like d' and criterion C) between Experiment 1 (novel foils only) and Experiment 2 (mixed foils) is crucial. This will allow for a quantitative assessment of the cognitive load imposed by the more complex foil environment. We expect overall accuracy to be lower and response times potentially longer in Experiment 2, reflecting the increased difficulty. This comparison will highlight the adaptive strategies, or their limits, when participants face greater mnemonic competition.
    
3. **Examine how different foil histories influence context utilization and retrieval strategies:** Beyond simple accuracy, we aim to understand if the presence of foils with prior experimental exposure alters participants' reliance on, or the nature of, contextual information used for recognition decisions. For instance, do participants adopt a more stringent decision criterion when they anticipate more confusable foils? Does the ability to retrieve list-specific contextual cues become more critical for performance? This aim involves exploring potential shifts in retrieval orientation or decision strategies as a function of the prevailing foil types.
    
4. **Further refine understanding of memory trace modification and long-term retention:** Experiment 2 continues to explore how encountering items in different capacities (target, specific types of foils) affects their subsequent memorability in the final, comprehensive test phase. By comparing the final test performance for items with these varied initial histories (e.g., an item initially a novel foil that was later re-presented as a foil from a prior list, versus an item that was only ever a novel foil), we can gain further insights into how testing events and interference sculpt the long-term representation of episodic memories.
    
5. **Provide a more stringent and diagnostically rich test for computational models of memory:** A core objective is to generate a complex, multi-faceted dataset that can rigorously test and further constrain the extended REM-based computational model developed in this dissertation. The model's ability to simultaneously account for performance with targets and the diverse array of foil types—capturing not just mean performance levels but also trends across lists and effects of specific item histories—will be a critical evaluation of its core assumptions regarding context representation, context drift, interference resolution mechanisms, and criterion setting in the decision process. This richer dataset will challenge the model to explain how context is dynamically updated and utilized to differentiate traces from numerous, potentially similar, prior episodes.
    

In essence, Experiment 2 aims to move beyond a basic understanding of context effects by creating an experimental environment that more closely mirrors the continual stream of overlapping experiences individuals navigate daily. By systematically introducing and tracking items with varied episodic histories, this experiment seeks to provide deeper insights into the adaptive mechanisms of human episodic memory, particularly its ability to leverage contextual information to resolve ambiguity and guide recognition decisions in the face of interference. The findings are expected to significantly inform our understanding of memory dynamics and provide critical data for advancing computational theories of context and memory.

### **4.2. Methodology**

#### **4.2.1. Participants**

Approximately 100 participants are planned for recruitment online via the Prolific platform ([www.prolific.com](https://www.prolific.com/ "null")). Prolific is a widely used platform for academic research, known for its diverse participant pool and tools that facilitate high-quality data collection.

- **Recruitment Criteria:** To ensure data quality and participant engagement, several pre-screening criteria will be applied on the Prolific platform. These will typically include a minimum approval rating from previous studies (e.g., 95% or higher), a minimum number of previously completed submissions (e.g., 50 or more) to ensure familiarity with online research environments, and native English language proficiency (as instructions and stimuli are in English). Age restrictions may be applied (e.g., 18-40 years) to target a specific adult population, or a broader range might be used depending on the specific research questions regarding age-related memory effects, though the latter is not a primary focus of this study. Participants who completed Experiment 1 will be excluded to ensure naivety to the general paradigm.
    
- **Power Analysis:** While a formal a priori power analysis for such a complex design with multiple item types and longitudinal effects is challenging, the target sample size of approximately 100 participants for the single final test condition is based on common practices in similar cognitive psychology experiments using online platforms, and is intended to provide sufficient power to detect medium-sized effects for key comparisons of interest, particularly differences in false alarm rates between critical foil types and changes in performance across lists. The within-subjects nature of the initial study-test phase also enhances statistical power for detecting effects related to item type and list progression.
    
- **Exclusion Criteria:** Data from participants will be excluded from the final analysis if they fail to meet certain performance criteria. These may include: (a) exceptionally low accuracy on the initial practice list (List 1), e.g., below 60% on targets or above 90% false alarms to novel foils, suggesting a lack of understanding or engagement; (b) excessively fast or slow mean response times across the experiment, potentially indicating automated responses or disengagement (e.g., mean RTs less than 300ms or greater than 3000ms for recognition judgments); (c) non-completion of the entire experiment; or (d) failure on embedded attention check trials, if any are included (e.g., specific instructions to press a certain key on a non-stimulus screen). The exact thresholds for these criteria will be pre-specified before data analysis.
    
- **Ethical Considerations:** The study protocol will be submitted to and approved by the [Name of Institutional Review Board (IRB) or Ethics Committee, e.g., University X Institutional Review Board] prior to data collection. Participants will provide informed consent electronically before starting the experiment. The consent form will detail the nature of the tasks, the estimated duration, data handling procedures (anonymity and confidentiality), compensation, and their right to withdraw at any time without penalty. All data collected will be anonymized by assigning participant ID codes, and no personally identifiable information will be stored with the experimental data. Upon completion, participants will receive a full debriefing explaining the purpose of the study in more detail. Compensation will be provided at a rate consistent with Prolific guidelines and the expected duration of the task.
    

#### **4.2.2. Materials and Stimuli**

The stimuli will consist of a large pool of color pictures depicting common, easily nameable objects, animals, and simple scenes.

- **Source and Nature:** Pictures will be sourced from [Specify source, e.g., "standardized image databases such as the Bank of Standardized Stimuli (BOSS) (Brodeur et al., 2010; 2014)," or "a curated selection from publicly available royalty-free image repositories like Pexels or Unsplash," or "custom-created stimuli"]. If multiple sources are used, consistency in style and quality will be aimed for. The use of pictures is intended to provide perceptually rich stimuli that can support robust episodic encoding.
    
- **Selection Criteria:** Stimuli will be selected to be: (a) clearly recognizable and relatively unambiguous; (b) generally neutral in emotional valence to avoid strong emotional reactions influencing memory disproportionately; (c) visually distinct from one another to minimize trivial perceptual confusion, although some degree of feature overlap is natural and expected; (d) suitable for presentation on a standard computer screen. Items with prominent text or highly culturally specific content will be avoided where possible to ensure generalizability.
    
- **Stimulus Pool Size:** A substantial pool of unique images is required to implement the complex design of Experiment 2. Given 10 lists, with 30 unique items studied per list (300 total studied items that are initially novel), plus the unique novel foils introduced in each list's test phase, and the large number of unique novel foils for the final test, the total number of unique images needed will be in the order of [e.g., "approximately 1000-1200 images"]. This large pool is essential to ensure that when an item is designated as "novel" (either as a studied item on its first appearance or as a novel foil), it has genuinely not been seen by the participant before in the experiment.
    
- **Pre-processing and Norming (if applicable):** All images will be standardized to a common size (e.g., 400x300 pixels or a similar moderate resolution suitable for web presentation) and file format (e.g., JPEG or PNG). While extensive individual norming for factors like familiarity, complexity, or name agreement for each image might be beyond the scope of this specific study, stimuli will be vetted by the researchers to ensure they meet the general selection criteria. If sourced from a database with existing norms, these might be considered during selection.
    
- **Counterbalancing of Stimuli:** To ensure that any observed effects are due to the experimental manipulations (e.g., item history, list number) rather than idiosyncratic properties of specific images, stimuli will be counterbalanced across participants and conditions. This means that a given picture will appear in different roles (e.g., as a target in one list for one participant, as a novel foil for another, or as a specific type of prior-exposure foil for yet another) across different participants. This is typically achieved by creating multiple assignment sets or by using a randomization algorithm that assigns images from the pool to roles dynamically for each participant, while respecting the design constraints (e.g., an item designated `S_Target_ToFoil` must indeed be available to serve as a foil in the subsequent list). The unit-based structure (3 items per unit) will aid in this counterbalancing, allowing units of images to be rotated through the different experimental roles.
    

#### **4.2.3. Design**

The experiment will employ a within-subjects design for the initial study-test phase and a single-condition (Random order) design for the final surprise test phase. The design is structured around "units" of items, where each unit typically consists of 3 unique pictures. This unit-based structure facilitates the counterbalancing and assignment of items to different roles across lists.

The critical manipulation in Experiment 2 is the composition of foils during the initial test phase, which includes items with varying prior exposures. Due to the complexity of item types and their flow across lists, a comprehensive diagram or a more detailed table illustrating this design, particularly the transitions of item types from one list to the next, would be a beneficial inclusion in the dissertation appendices. The summary tables below outline the core composition of each phase.

To maintain clarity, the following consistent terminology will be used for item types based on their history in relation to list _n_:

- **Studied Items (Presented in Study Phase of List** _**n**_**):**
    
    - `S_Target_End`: Studied in list _n_, subsequently tested as a **Target** in list _n_. This item type is not designated for reuse as a foil in list _n+1_. Its experimental history, for the purpose of becoming a lure, effectively ends after being a target in list _n_.
        
    - `S_Target_ToFoil`: Studied in list _n_, subsequently tested as a **Target** in list _n_, and then designated to serve as a **Foil** (specifically, as an `F_PriorS_Target` type) in the test phase of list _n+1_.
        
    - `S_NonTested_End`: Studied in list _n_ but **Not Tested** as a target in list _n_. This item type is not designated for reuse as a foil in list _n+1_. Its history as a potential lure also effectively ends.
        
    - `S_NonTested_ToFoil`: Studied in list _n_ but **Not Tested** as a target in list _n_, and then designated to serve as a **Foil** (specifically, as an `F_PriorS_NonTested` type) in the test phase of list _n+1_.
        
- **Novel Foil Items (First presented in Test Phase of List** _**n**_**):**
    
    - `F_Novel_End`: Presented as a **Novel Foil** in list _n_. This item type is not presented again in any capacity during the initial study-test lists.
        
    - `F_Novel_ToFoil`: Presented as a **Novel Foil** in list _n_, and then designated to serve as a **Foil** again (specifically, as an `F_PriorF_Novel` type) in the test phase of list _n+1_.
        
- **Prior-Exposure Foil Items (Presented as Foils in Test Phase of List** _**n**_**, having history from list** _**n-1**_**):**
    
    - `F_PriorS_Target`: A foil in list _n_ that was an `S_Target_ToFoil` item from list _n-1_.
        
    - `F_PriorS_NonTested`: A foil in list _n_ that was an `S_NonTested_ToFoil` item from list _n-1_.
        
    - `F_PriorF_Novel`: A foil in list _n_ that was an `F_Novel_ToFoil` item from list _n-1_.
        

##### **4.2.3.1. Initial Study-Test Phase**

This phase will consist of 10 distinct study-test lists.

- **Study Trials (All Lists):** For each list _n_ (from 1 to 10), participants will study 10 units of unique pictures (totaling 30 items), presented one at a time. These items are all new to the experiment at the point of their first study. The assignment of specific pictures to these roles will be counterbalanced across participants. (See Table 4.1 for composition).
    
- **Inter-Phase Distractor:** As in Experiment 1, a distractor task (e.g., approximately 17 seconds of counting visually presented single digits) will occur between the study and test trials for each list. This serves to minimize immediate rehearsal from the study phase and to allow for some temporal separation and potential context drift between study and test.
    
- **Test Trials (List 1):** Immediately following the distractor task, participants will be presented with 10 units of recognition test items (30 items) for List 1. This list serves as a baseline and a practice for the subsequent, more complex lists. (See Table 4.2 for composition). The order of targets and foils will be randomized for each participant.
    
- **Test Trials (Lists 2-10):** For each list _n_ (from 2 to 10), participants will be presented with 10 units of recognition test items (30 items). The critical manipulation here is the introduction of foils with prior experimental histories. (See Table 4.3 for composition). The order of targets and these mixed foils will be randomized for each list and participant. The selection of specific items from list _n-1_ to serve as prior-exposure foils in list _n_ will be determined by their assigned role in list _n-1_ (e.g., an item designated `S_Target_ToFoil` in list _n-1_ becomes an `F_PriorS_Target` foil in list _n_).
    

##### **4.2.3.2. Final Test Phase**

After completing all 10 initial study-test lists, participants will engage in a final, surprise test phase.

- **Items Tested:** A total of 246 unique "OLD" items, encountered during the initial study-test phases, will be presented. These OLD items are selected by taking 2 items from each of 123 specific units: 15 units representing experiences from List 1 (covering its studied items and initial foils) and 12 units representing experiences from each of Lists 2 through 10. An additional 246 pictures, entirely new to the experiment, will serve as novel foils. Thus, the final test will consist of 492 trials (246 OLD, 246 NEW). The precise mapping of which 123 units are chosen for the final test from the larger pool of experienced units would be detailed based on the experimental design's counterbalancing. _[random select..? Placeholder for specific sampling strategy of units for final test items]._
    
- **Test Condition:** All participants will receive the final test items in a pseudo-random order, intermixing all OLD and NEW items.
    

#### **4.2.4. Procedure**

The general procedure will mirror Experiment 1, with adjustments for the different item structures. Participants will complete the experiment online using an interface programmed in JavaScript with jsPsych.

- **Initial Study-Test Phase:**
    
    - For each of the 10 lists:
        
        - Study phase: Presentation of 30 pictures (10 units) (e.g., 3 seconds per picture, 1-second ISI).
            
        - Distractor task: ~17 seconds.
            
        - Test phase: Presentation of 30 pictures (10 units: 5 units of targets, 5 units of foils as per list number). "Old"/"New" judgments required within a time limit (e.g., 3.5 seconds). Pictures presented with a short ISI (e.g., 100ms).
            
    - Feedback during the initial test phase will be [Specify feedback type: e.g., accumulated accuracy bar per list, as in E1, or trial-by-trial real-time feedback after subjects make each decision].
        
- **Final Test Phase:**
    
    - Instructions for the surprise final test.
        
    - Presentation of 492 items (246 OLD, 246 NEW) in a random order. "Old"/"New" judgments required. Presentation timings and response requirements consistent with initial tests.
        
    - Feedback during the final test phase will be [Specify feedback type: e.g., "This item had been studied/tested on list XX" or "This item had never been seen previously," as in E1, adapted for the more complex item histories if necessary].
        

Upon completion, participants will be debriefed. The experiment code, analysis code, and modeling code for Experiment 2 will be made available via [Placeholder for open-resource git link]. The estimated duration of the experiment is approximately 45 minutes.
## tables here

**Table 4.1: Composition of Study Phase (Lists 1-10)**
*Each Study Phase consists of 10 units (30 items total).*

| Item Type | Number of Units | Number of Items |
| :--- | :--- | :--- |
| `S_Target_End` | 4 | 12 |
| `S_Target_ToFoil` | 1 | 3 |
| `S_NonTested_End` | 4 | 12 |
| `S_NonTested_ToFoil` | 1 | 3 |
| **Total Studied** | **10** | **30** |
**Table 4.2: Composition of Test Phase for List 1**
*Test Phase for List 1 consists of 10 units (30 items total).*

| Category             | Item Type            | Number of Units | Number of Items |
| :------------------- | :------------------- | :-------------- | :-------------- |
| Targets              | `S_Target_End`       | 4               | 12              |
|                      | `S_Target_ToFoil`    | 1               | 3               |
|                      | **Subtotal Targets** | **5**           | **15**          |
| Foils                | `F_Novel_End`        | 4               | 12              |
|                      | `F_Novel_ToFoil`     | 1               | 3               |
|                      | **Subtotal Foils**   | **5**           | **15**          |
| **Total Test Items** |                      | **10**          | **30**          |
**Table 4.3: Composition of Test Phase for Lists 2-10**
*Each Test Phase for Lists 2-10 consists of 10 units (30 items total).*

| Category | Item Type | Number of Units | Number of Items |
| :--- | :--- | :--- | :--- |
| Targets | `S_Target_End` (current list *n*) | 4 | 12 |
| | `S_Target_ToFoil` (current list *n*) | 1 | 3 |
| | **Subtotal Targets** | **5** | **15** |
| Foils | `F_Novel_End` (new for list *n*) | 1 | 3 |
| | `F_Novel_ToFoil` (new for list *n*) | 1 | 3 |
| | `F_PriorS_NonTested` (from list *n-1*) | 1 | 3 |
| | `F_PriorS_Target` (from list *n-1*) | 1 | 3 |
| | `F_PriorF_Novel` (from list *n-1*) | 1 | 3 |
| | **Subtotal Foils** | **5** | **15** |
| **Total Test Items** | | **10** | **30** |