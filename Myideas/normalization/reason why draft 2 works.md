# **TVA assumes that each object has its own _independent attentional weight_**

and that these weights are **scalars**, not functions over a space.

### TVA assumption:

Each object xxx has a weight:

wx=Î·xÏ€xw_x = \eta_x \pi_xwxâ€‹=Î·xâ€‹Ï€xâ€‹

No feature space is modeled.  
Objects do _not_ occupy regions in a representational space.  
Weights simply add:

âˆ‘wz=w1+w2+â€¦\sum w_z = w_1 + w_2 + \dotsâˆ‘wzâ€‹=w1â€‹+w2â€‹+â€¦

This implicitly assumes:

> **Object representations are non-overlapping, independent units.**

---

# ðŸŒ **2. Your model introduces a different assumption:

Objects occupy measurable _regions_ in a representational space**

Objects are no longer independent scalars; they are **sets** in feature space:

ExâŠ‚Î©RE_x \subset \Omega_RExâ€‹âŠ‚Î©Râ€‹

with a measure:

Î½(Ex)\nu(E_x)Î½(Exâ€‹)

Once objects have spatial extent in representational space, the following becomes _inevitable_:

Î½(E1âˆªE2)=Î½(E1)+Î½(E2)âˆ’Î½(E1âˆ©E2)\nu(E_1 \cup E_2) = \nu(E_1) + \nu(E_2) - \nu(E_1 \cap E_2)Î½(E1â€‹âˆªE2â€‹)=Î½(E1â€‹)+Î½(E2â€‹)âˆ’Î½(E1â€‹âˆ©E2â€‹)

Thus:

> **Sub-additivity is not an assumption; it is a _mathematical consequence_ of treating objects as overlapping sets rather than independent scalars.**

This is the single, fundamental shift that makes your new prediction possible.

---

Your internal operations are:

$$wx=âˆ«ExÏ€(Ï‰)dÎ¼w_x = \int_{E_x} \pi(\omega)d\mu wxâ€‹=âˆ«Exâ€‹â€‹Ï€(Ï‰)dÎ¼$$

and

$$vx=Câ‹…wxÎ½(âˆªEz)v_x = C \cdot \frac{w_x}{\nu(\cup E_z)}vxâ€‹=Câ‹…Î½(âˆªEzâ€‹)wx$$â€‹â€‹