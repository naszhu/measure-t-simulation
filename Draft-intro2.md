I will begin by outlining the core mechanisms of REM, starting with the simplest formulation presented by Shiffrin and Steyvers (1997). This foundation is important because the later elaborations of the model—including the differentiation of feature types, the dynamics of context change, the updating of traces at test, and the contribution of recall—are all layered on top of these basic assumptions. By first presenting the elementary operations of storage, comparison, and decision, I can show how the defining principles of REM give rise to the major regularities of recognition memory, while also connecting to the broader global-matching tradition. After that, I will move step by step through the model’s further developments: the role of contextual and content features, the implementation of drift and reinstatement, the addition of new traces during testing, the dual influence of familiarity and recall, and the consequences of strengthening and interference across a test. In this way, the discussion proceeds from the most general commitments of REM to the more complex processes needed to account for the full range of empirical findings.
## I. Core processes in simple REM

The REM model in its simplest form (Shiffrin & Steyvers, 1997) specifies a compact set of mechanisms that have come to be regarded as fundamental to recognition memory. These mechanisms define both the representational assumptions of the model—how episodic traces are encoded and stored—and the decision processes by which recognition judgments are made. In REM, traces are stored noisily and probabilistically, and test probes are compared to all traces through a likelihood-ratio global matching process in which evidence is weighted by feature diagnosticity. The resulting familiarity signal is defined as the average likelihood ratio across traces, providing the basis for recognition decisions (Shiffrin & Steyvers, 1997; Montenegro, Myung, & Pitt, 2014). Together, these principles offer a unified framework that explains hallmark regularities of recognition such as the mirror effect and list-length and list-strength phenomena, while maintaining continuity with the broader global-matching tradition (Clark & Gronlund, 1996; Starns, White, Ratcliff, & Rouder, 2010).
### **1. Noisy storage of features.**

A central process in REM is the assumption that episodic traces are stored noisily. When an item is studied, only a subset of its features is encoded, and those that are stored may be inaccurate or corrupted (Shiffrin & Steyvers, 1997). Each trace is therefore a probabilistic, degraded record rather than a straight copy. This principle has a long lineage: formal models like SAM (Gillund & Shiffrin, 1984; Raaijmakers & Shiffrin, 1981) and MINERVA (Hintzman, 1984, 1986) already treated episodic encoding as partial and error-prone. REM formalized this idea mathematically, specifying that each feature is stored with probability less than one and may be replaced by an incorrect value drawn from environmental base-rate distributions (Shiffrin & Steyvers, 1997). As a result, repeated presentations of the same item produce overlapping but non-identical traces, ensuring that memory contains variability rather than perfect replicas. This stochastic encoding is essential for recognition because it creates the variability across traces that produces overlapping familiarity distributions for studied and novel items; in REM, those distributions are derived mechanistically from assumptions about noisy sampling, rather than posited ad hoc (Montenegro et al., 2014).

### **2. Diagnosticity-Weighted Global Matching in REM**

A second component in REM is the treatment of diagnosticity. Earlier global-similarity models such as SAM (Gillund & Shiffrin, 1984) and MINERVA (Hintzman, 1984, 1986) often treated features as contributing more uniformly to recognition decisions. REM instead assigns evidential weight to features according to their informativeness, distinguishing two complementary sources of diagnosticity.

At the **environmental** level, features differ in their base rates: common features are less informative, while rare features are highly diagnostic. By assuming that feature values are sampled from base-rate distributions, REM formalizes this idea so that matches on rare features yield larger likelihood ratios than matches on common ones (Shiffrin & Steyvers, 1997; see also Montenegro et al., 2014, for analytic expressions). This weighting is crucial for explaining robust phenomena such as the mirror effect, where low-frequency items produce both higher hit rates and lower false alarms (Glanzer & Adams, 1985; Starns et al., 2010).

At the **task** level, diagnosticity also depends on the similarity structure of the particular experimental set. When items within a list share many overlapping attributes, only a subset of features will distinguish among them, and these become especially diagnostic in the task context; conversely, with highly heterogeneous sets, even moderately common features may retain discriminative value. REM therefore incorporates both environmental and task diagnosticity into its evidence calculations. This makes recognition responsive to both long-run environmental statistics and the distinctive structure of the current study set (Shiffrin & Steyvers, 1997; Clark & Gronlund, 1996; Criss, 2006).

### **3. Parallel Probe–Trace Comparison in REM**

Another central process in REM is the parallel comparison of the probe to all stored traces. This principle has its roots in the global-matching tradition where recognition reflects the combined influence of all traces rather than retrieval of a single best match (e.g., Gillund & Shiffrin, 1984; Hintzman, 1984, 1986; Clark & Gronlund, 1996)**. Global comparison was introduced to explain why memory judgments are graded and probabilistic, and why performance is sensitive to list composition: a probe is influenced not only by its own trace but also by similarity to other stored items.** REM adopts this assumption but reformulates it within a likelihood-ratio framework, specifying that every trace contributes evidence according to the probability that its feature values could have generated the probe’s features under “old” versus “new” hypotheses; the resulting per-trace likelihood ratios are then combined (Shiffrin & Steyvers, 1997; Montenegro et al., 2014).

The importance of this parallelism is twofold. First, it grounds recognition in the full distribution of memory, allowing interference and variability across episodes to systematically affect recognition outcomes (Clark & Gronlund, 1996; Criss, 2006). Second, by computing likelihood ratios for each trace and then integrating them, REM translates the intuitive idea of global similarity into a principled Bayesian evaluation of the memory store (Shiffrin & Steyvers, 1997). In this way, the parallel comparison mechanism ensures that recognition decisions reflect the combined influence of all stored traces, capturing interference and variability across episodes; recent large-scale tests emphasize that compact REM implementations predict wide arrays of conditions without heavy re-tuning (Mohamed et al., in press).

### 4. **Familiarity as the average likelihood ratio.**

REM defines familiarity as the decision variable that follows from the parallel comparison process. After the probe is compared to all traces, the resulting likelihood ratios are averaged to compute a global “odds” value, which serves as the measure of familiarity (Shiffrin & Steyvers, 1997). A decision criterion is applied to this average: if the odds exceed *certain value* (*or 1*? *CAUTION here*) , the item is judged old; otherwise, it is judged new. This Bayesian derivation makes the decision process optimal in the sense that it uses diagnostic information available in memory traces for distinguishing targets from foils (Shiffrin & Steyvers, 1997; Montenegro et al., 2014).

The importance of defining familiarity in this way is twofold. First, it embeds recognition within a normative likelihood-based framework, offering a principled account of why familiarity judgments track discriminability across conditions such as list length and word frequency (Shiffrin & Steyvers, 1997; Starns et al., 2010). Second, it links REM to broader recognition paradigms traditionally described with signal detection theory. Whereas SDT treats familiarity as a latent strength sampled from overlapping distributions (Green & Swets, 1966; Macmillan & Creelman, 2005), REM derives the familiarity signal from concrete feature–trace comparisons, showing how those distributions arise mechanistically from storage noise and diagnosticity. This mechanistic account allows REM to generalize across formats, including old–new tests, two-alternative forced choice, and more complex multi-item judgments (Mohamed et al., in press).

Critically, this conception of familiarity as an average likelihood ratio explains why recognition can succeed without explicit recall of episodic details—a phenomenon long noted in remember–know paradigms and dual-process discussions (Mandler, 1980; Yonelinas, 2002). Even when no specific trace is retrieved, the accumulated similarity signal provides a robust probabilistic basis for decision. In this sense, REM formalizes a process that earlier models such as SAM described heuristically, and demonstrates that a familiarity-only decision rule—appropriately defined—can capture major qualitative patterns of recognition (Gillund & Shiffrin, 1984; Shiffrin & Steyvers, 1997; Clark & Gronlund, 1996).

## II. Differentiation of Different Feature Types: Context Features and Content Features

A further development of REM and its extensions is the explicit differentiation of feature types—contextual versus content features—and their distinct functional roles in recognition. This distinction was implicit in earlier global-matching models but became explicit in REM and subsequent elaborations (Murnane & Shiffrin, 1991a, 1991b; Shiffrin & Steyvers, 1997).

Content features refer to attributes tied directly to studied items—semantic, lexical, or perceptual properties that differentiate one stimulus from another. In REM, these features are the primary basis for fine-grained discrimination among test probes once the relevant context is accessed. Each item contributes a set of content features stored with some probability (e.g., copying parameter _c_, storage probability _u_), and these features are diagnostic to the extent that they differ across items. During testing, similarity on content features generates likelihood ratios that directly drive familiarity-based recognition judgments.

By contrast, context features represent attributes that bind items to their study episode—temporal position, list membership, task environment, or situational cues (Howard & Kahana, 2002; Polyn et al., 2009). In REM and its descendants, context features serve primarily to filter which traces are activated for evaluation. REM.4 and later variants introduced context-based thresholds, ensuring that only traces with sufficient contextual match contribute to recognition (Shiffrin & Steyvers, 1997).

Both theory and empirical findings indicate that context in memory is not a single, uniform construct. Instead, it comprises subtypes that differ in temporal stability and functional role. Early experimental work on context-dependent memory (e.g., Godden & Baddeley, 1975; Murnane & Shiffrin, 1991a, 1991b) highlighted that some contextual cues remain relatively constant across episodes—such as environmental setting or task instructions—while others shift more dynamically with time, lists, or intervening events. Computational models formalize this distinction. For example, the Temporal Context Model (Howard & Kahana, 2002) treats temporal context as a gradually drifting state vector, whereas REM retains both stable background features and dynamic, list-specific ones (Shiffrin & Steyvers, 1997).

Within this framework, two broad classes of context features can be identified.

1. Unchanging context features remain stable across lists, providing a consistent background that links episodes together. *Although the precise form of this stable context may differ across tasks,* (*CATION: mention here about changes in unchanging context or not*) its persistence allows recognition judgments to generalize across temporal boundaries by anchoring items to a common frame of reference.
    
2. Changing context features vary systematically across study lists or drift gradually within a list. These dynamic cues help partition memory into distinct episodes, thereby reducing interference between lists. At the same time, because drift is incomplete, partial overlap remains, which explains why proactive interference and context reinstatement effects can coexist (Howard & Kahana, 2002; Hanczakowski, Beaman, & Jones, 2015).
    

Recognition in REM proceeds in two sequential stages that make direct use of this distinction. First, context filtering: both stable and dynamic features of the probe are compared with stored traces, and only those traces exceeding a context threshold are activated. Stable features help maintain broad continuity, while dynamic features restrict retrieval to traces plausibly belonging to the current episode. Second, content evaluation: within the activated subset, content features are compared for similarity, and likelihood ratios are computed relative to distractors to produce a familiarity signal.

This dual-role framework—stable context anchoring memory across episodes, dynamic context differentiating between them—provides a natural explanation for otherwise puzzling findings. Improved recognition under context reinstatement (Murnane & Shiffrin, 1991a) follows from the filtering role of dynamic context, while list-length and list-strength effects can be understood as the joint product of context-based filtering and content-based item differentiation (Ratcliff, Clark, & Shiffrin, 1990; Shiffrin, Ratcliff, & Clark, 1990).

Another key principle is that context may not remain entirely static during study but instead changes gradually as the study phase progresses. This idea, often referred to as context drift, has a long history in memory theory (Estes, 1955) and is central to modern computational accounts such as TCM (Howard & Kahana, 2002) and REM-based extensions (Shiffrin & Steyvers, 1997). As new items are studied, context drifts incrementally, reflecting both the passage of time and the cumulative effect of encoding. This drift supports differentiation between lists but, because it is incomplete, also produces partial overlap and interference. Empirical findings confirm this principle: recognition accuracy decreases when intervening study produces contextual drift, and reinstating study context can partially restore performance (Murnane et al., 1999; Hanczakowski et al., 2015). Output interference in recognition tests—where later test items are less accurately judged—has likewise been linked to reduced precision of contextual reinstatement (Criss, Malmberg, & Shiffrin, 2011).

Within REM, context change is implemented as the probability that a subset of context features differs between study and test or across successive lists. By incorporating this controlled form of change, REM is able to account for list-length effects, proactive interference, and the benefits of reinstating study context. In this way, context is not a static backdrop but an evolving representation that both constrains and shapes recognition across time.

*(write how changing and unchanging context change in modeling part or here?)*

## III. Storage of New Traces at Study and at Test

*(CATION: I'm not sure about this section, in which way, to explain storage of every traces, including those recalled or not)*

In the original REM formulation, new episodic images were assumed to be stored only during study, with no additional storage at test (Shiffrin & Steyvers, 1997). Later extensions broadened this principle, incorporating the idea that recognition itself can modify memory. In these models, a test probe judged “old” strengthens the corresponding trace, whereas a probe judged “new” leads to the creation of a new trace (Criss, Malmberg, & Shiffrin, 2011). This generalization from study to test provided a natural account of output interference—the systematic decline in recognition performance across test positions—by showing how every recognition act leaves memory in a slightly altered state.

In the present model, storage occurs for all test probes, regardless of whether they are correctly recognized, misclassified, or even foils. Two mechanisms are distinguished. First, strengthening applies selectively to content features of previously studied items: when a target is judged old, its most likely stored trace is retrieved, and blank or mismatched content features can be restored, making the trace less confusable with lures. Second, addition applies to both targets and foils: every test probe, even when judged new, generates a new trace with content features stored under the same noisy rules used at study. Importantly, this updating process is asymmetric across feature types—context features are not strengthened during test, but content features are, while both can be copied into newly added traces. Moreover, target items receive privileged encoding (probability of storage and copying set to 1), whereas foils are encoded at much lower probabilities, introducing systematic differences between these categories.

By assuming that every recognition probe results in either strengthening or addition, the model implements a continuous-update framework. Old items can accumulate multiple traces or be restored at their content features; foils become sources of later interference if reencountered; and recognition decisions are increasingly influenced by this growing population of traces as the test unfolds. This assumption captures both the benefits of testing (e.g., reduced confusability for strengthened targets) and its costs (e.g., test-potentiated false alarms from stored foils), embedding recognition within a dynamic cycle in which retrieval and encoding operate simultaneously to shape memory.

## IV. Recall of Traces Supplementing (and Sometimes Opposing) Familiarity for Recognition Decisions

Although REM provides a principled account of recognition as a familiarity-based process, there is strong evidence that recall-like retrieval operations can also contribute to recognition. These processes can supplement familiarity when additional detail is needed, and in some cases they can even oppose the familiarity signal by enabling participants to reject lures that otherwise feel familiar.

Two key recall-like mechanisms are often invoked in recognition memory:

- **Recall-to-accept:** This process refers to the retrieval of a matching trace that supplements familiarity by primarily strengthening responses to studied items, rather than systematically influencing foils. It strengthens the acceptance of studied items by confirming their episodic details (Rotello & Heit, 2000). Although theoretically plausible, it has received less empirical attention than recall-to-reject.
    
- **Recall-to-reject:** This process operates when a test probe is highly similar to a studied item, such that familiarity alone cannot reliably discriminate between them. By retrieving the actual studied trace, participants can detect a mismatch between the probe and memory, leading to a “new” judgment despite the probe’s familiarity. This mechanism is particularly important in associative recognition and source memory tasks, where lures share many features with targets (Rotello & Heit, 2000; Yonelinas, 2002). Evidence for recall-to-reject includes reduced false alarms to similar foils when sufficient time is available for retrieval and distinctive ROC patterns in associative recognition (Rotello & Heit, 2000; Rotelloet al., 2000). The general idea that recollection can oppose familiarity also appears in process dissociation methods (Jacoby, 1991), in dual-process frameworks where recollection supports high-confidence rejections (Yonelinas, 2002), and in integrated theory, which highlights recall-to-reject as central to reducing false alarms in associative and plurality discrimination tasks (Malmberg, 2008). Together, these findings show that recall-to-reject provides a corrective function in recognition: it enables participants to override misleading familiarity signals by drawing on episodic detail.

In both cases, recall acts as a strategic supplement to the familiarity calculation, functioning as a second-stage retrieval process that can confirm or override the outcome of the familiarity judgment.

This dual contribution helps explain phenomena that familiarity alone cannot capture:

Recall processes help explain recognition outcomes that familiarity alone cannot capture. High-confidence rejections of similar lures arise when recall-to-reject overrides misleading familiarity (Yonelinas, 2002). In associative and source recognition, recollection binds item–item or item–context relations, reducing false alarms when lures overlap with targets (Rotello & Heit, 2000; Malmberg, 2008). Recall may also contribute to the mirror effect: Low-frequency items show higher hits and lower false alarms not only because their features are more diagnostic (as original REM emphasizes), but also because they are easier to recollect episodic details, since their distinctiveness reduces cue overload and interference (Glanzer & Adams, 1985; Malmberg et al., 2004). Together, these findings highlight recall’s corrective role in strengthening target acceptance and limiting lure errors.

Evidence for recall contributions comes from multiple paradigms. Remember/Know judgments show that participants distinguish between familiarity-based “knowing” and detail-based “remembering,” with corresponding behavioral and neural markers (Mandler, 1980; Yonelinas, 2002). Associative and source recognition tasks also demonstrate that performance improves when recollection is possible, particularly by reducing false alarms to related lures (Rotello & Heit, 2000). Similar conclusions emerge from process dissociation procedures, where recollection is shown to oppose familiarity and enable correct rejection of items that would otherwise seem old (Jacoby, 1991).

Classic REM focuses on familiarity, but extensions of the model incorporate recall-like processes in order to capture these richer patterns of recognition. One approach is to posit a second threshold that, when exceeded, triggers retrieval of episodic details from one or more traces. These recalled details can then be used to strengthen acceptance of a true target or to identify inconsistencies in the case of a lure, leading to rejection. This layered decision architecture mirrors empirical evidence that recognition judgments are not purely familiarity-driven but can involve strategic, detail-based retrieval.

In this way, recall functions as both a supplement and a corrective: it can increase hits by confirming item-specific details and decrease false alarms by exposing mismatches. By embedding recall processes within the broader REM framework, recognition memory can be modeled as the joint outcome of fast, global familiarity signals and slower, detail-oriented recall mechanisms.

## V. Strengthening of Recalled Traces

A further process in recognition memory is the strengthening of traces when they are recalled. The very process of recalling an item can modify its underlying representation, making it more accessible in subsequent memory judgments. This principle links recognition to the broader literature on the testing effect (Roediger & Karpicke, 2006), where retrieval itself enhances later memory performance.

When a probe triggers the successful recall of a trace, the recovered details can be re-encoded into memory, filling in previously missing features or increasing the probability that key features will be stored. In REM-style implementations, this is modeled as an increase in the probability of *storing blank or mismatched features* (*comment: be cautious the final model should be aligned to what written here*) once recall succeeds. In this way, recalled traces become more complete and more robust representations after retrieval.

The strengthening of recalled traces serves two complementary roles: It boosts recognition of true targets on later encounters , because their traces have been enriched with additional features from the recall episode. It also increases susceptibility to output interference, because repeated strengthening of earlier test items can shift the relative diagnosticity of traces, making later judgments more difficult. In other words, recall contributes not only to improved retention but also to the competitive dynamics that produce declining accuracy across a test list. (*I'm not sure if  saying in this way is good because it gets complicated when how the recall of list origin is used in rejection. But this is true for final test )*

A rich body of research shows that memory retrieval enhances subsequent performance: retrieving information during testing improves long-term retention more than re-study alone (Roediger & Karpicke, 2006). Within recognition specifically, test-potentiated learning has been observed: items tested once are remembered better later, even without feedback (Carrier & Pashler, 1992). At the same time, recall-based strengthening contributes to output interference—performance tends to decline across a sequence of test trials, reflecting the cumulative effects of strengthening and competition (Criss et al., 2011).

Within REM, strengthening is implemented as a higher storage probability for features of recalled traces. When an item is judged “old” and its trace is successfully retrieved, blank or incorrect features can be overwritten with correct values, increasing the completeness of the trace (Shiffrin & Steyvers, 1997). This mechanism allows the model to represent how successful recall episodes enrich stored traces and make them more diagnostic for subsequent recognition.

## VI. Output Interference 

A further process shaping recognition performance is **output interference**—the systematic decline in recognition accuracy as a test session progresses. This phenomenon emerges directly from two mechanisms already described: **the storage of new traces at test** and **the strengthening of recalled traces**. While the original REM framework (Shiffrin & Steyvers, 1997) focused on core recognition phenomena such as list-length, list-strength, and mirror effects, subsequent extensions of REM and related multiple-trace approaches have built on this foundation to account for within-test performance dynamics. In these models, output interference follows naturally once test probes are assumed to contribute new traces or strengthen existing ones, thereby influencing recognition decisions later in the session (Hintzman, 1984; Starns et al., 2010; Criss, Malmberg, & Shiffrin, 2011).

In these models, each test trial can modify memory: when an item is probed, a new trace may be added, and if the item is judged old, its prior traces may be strengthened. Over the course of testing, the memory store therefore includes both newly stored foils and strengthened targets. This evolving representation changes the evidence available for later recognition decisions. For example, foils introduced earlier in the test can raise false-alarm rates if re-encountered, while strengthening of early targets can reduce the relative distinctiveness of later ones (Criss et al., 2011).

This mechanism produces a serial position effect within the test: accuracy tends to be higher for early test items and declines for items probed later. Such declines emerge naturally if the act of testing alters memory, but they are not predicted by static models that assume memory remains fixed during test.

Evidence for output interference has been found across multiple paradigms. Malmberg and Shiffrin (2005) observed increased false alarms to foils later in the test sequence, consistent with foil storage. Criss, Malmberg, and Shiffrin (2011) reported robust decreases in recognition performance across test positions even when study conditions were equated, indicating that the test itself introduces interference. These findings converge with earlier demonstrations that the act of retrieval can reshape memory in ways that affect subsequent judgments (Roediger & Karpicke, 2006).

By incorporating test-storage and strengthening, extended REM accounts for output interference as an emergent property of dynamic updating: every recognition judgment both samples from and reshapes the memory store. This view aligns REM with other multiple-trace approaches (e.g., MINERVA 2), where test probes are encoded as additional traces (Hintzman, 1984). In contrast, models that do not allow test-driven updating must invoke separate mechanisms to explain the within-test decline.

In this way, output interference highlights a broader principle: recognition tests are not purely diagnostic of a fixed memory state but are themselves part of the learning process, altering the structure of memory even as they measure it.

## VII. Selection of Appropriate Context to Probe Memory

In addition to storing traces with both content and context features, recognition also requires selecting which mixture of context features to use when probing memory. Because the context representation includes both _changing_ and _unchanging_ components, the memory system can be probed with different ratios of these two feature types. The ratio determines how restrictive or flexible the search will be.

When the probe context emphasizes unchanging features, the search process draws broadly across lists, capturing long-term stability but increasing the chance of interference from prior episodes. When the probe context emphasizes changing features, the search is narrowed to a particular list or temporal segment, reducing interference but also making retrieval more sensitive to drift and reinstatement errors. By tuning the ratio of changing to unchanging features, the model can flexibly balance stability and distinctiveness in recognition.

This mechanism captures the intuition that recognition judgments often depend on how precisely one reinstates the study context. Strong reliance on stable features supports long-term retention across sessions, while reliance on changing features allows differentiation among closely spaced episodes. For example, in the final test phase of the current model, the probe context is constructed to emphasize 100% unchanging and 30% (*CAUTION: make sure the number aligns*) changing features, reflecting the need for stability over long retention intervals while still maintaining some sensitivity to recent list structure.

Experimental work on context reinstatement demonstrates that recognition performance depends on the specificity with which contextual information is reinstated. For example, reinstating list-specific context reduces false alarms to between-list foils (Murnane & Shiffrin, 1991b). Other findings show that when reinstatement is more general, memory access can be maintained across episodes but at the cost of greater interference (Hanczakowski, Beaman, & Jones, 2015). In the present model, this trade-off is formalized by varying the ratio of changing to unchanging context features used in the probe.

In the present model, context selection is formalized by representing the probe context as a weighted mixture of changing and unchanging features. The weighting is specified a priori for the analyses reported and is held constant within a given modeling specification. The resulting composite probe is then compared with stored traces. This formulation captures the idea that people may rely to different degrees on relatively stable versus dynamic contextual information without positing a discrete filter or online reweighting at the level of individual items (cf. Murnane & Shiffrin, 1991a; Howard & Kahana, 2002).