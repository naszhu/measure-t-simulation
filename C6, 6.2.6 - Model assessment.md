### 6.2.6 Model Assessment / Discussion

The preceding section detailed the architecture and predictive successes of the SLR23 model. This section delves into an assessment of the model's quality, focusing on the justification for its parameterization, the reasonableness of the estimated parameter values, and a critical comparison with its predecessor, the SLR21 model (Nosofsky et al., 2021). As emphasized by Lai, Cao, and Shiffrin (2024), the evaluation of a model intended to account for such a broad and complex dataset hinges not merely on quantitative fit, but also on its theoretical coherence, parsimony, and the conceptual plausibility of its components and their estimated contributions.

#### Model Flexibility and Parameter Variations

Developing a computational model to account for accuracy and response time data across 376 distinct conditions, derived from 13 participant groups tested in multiple studies over several years, presents a formidable challenge. A key objective in the development of SLR23 was to achieve this breadth of coverage while maintaining a principled approach to model complexity and parameter flexibility (Lai, Cao, & Shiffrin, 2024). The authors explicitly sought a model that could capture the diverse empirical patterns with a minimal and justifiable set of parameter variations.

The SLR23 model, while comprehensive, was structured to ensure that most of its core parameters remained invariant across the wide range of experimental manipulations and participant cohorts. The variations that were permitted were introduced to account for specific, theoretically anticipated sources of variance:

1. **Base Times (**τ0​**)**: This parameter, representing non-decisional components of response time (e.g., stimulus encoding, motor execution), was allowed to vary across the 13 distinct participant groups. This is a standard practice in evidence accumulation modeling, acknowledging that baseline processing speeds can differ systematically between groups tested at different times or with slightly different procedural nuances (Lai, Cao, & Shiffrin, 2024, p. 17, 20).
    
2. **Learning Rates (**Λj​**)**: The strength of the learning component, Λj​, was allowed to differ across the eight distinct Consistent Mapping (CM) conditions. This flexibility was deemed necessary because the opportunity for, and nature of, learning varied substantially. For example, learning was expected to be more robust in pure CM blocks compared to mixed-list conditions where CM items were less frequent. Furthermore, the novel CM paradigm introduced in Lai, Cao, and Shiffrin (2024) involved a more complex stimulus-response-key mapping than the standard CM paradigm in Nosofsky et al. (2021), potentially leading to different learning rates (Lai, Cao, & Shiffrin, 2024, p. 20).
    
3. **Short-Term Mismatch Similarity in Stage 2 (**Ω2XX​**)**: Six distinct values were estimated for the similarity between a probe and a mismatching short-term memory (STM) trace during the second stage of processing. These values depended on the types of items involved in the mismatch (i.e., combinations of All-New (AN), Varied Mapping (VM), and CM items for the probe and the trace). The rationale for this variation stems from the hypothesis that participants may differentially encode or attend to AN, VM, and CM stimuli due to their distinct roles and predictive values within the experimental context. Such differential encoding could, in turn, affect the perceived similarity between mismatching items, a concept with precedent in exemplar models of categorization (e.g., Nosofsky, 1986, as cited in Lai, Cao, & Shiffrin, 2024, p. 19, 22).
    

Despite these targeted variations, Lai, Cao, and Shiffrin (2024, p. 20) emphasize that the majority of the model's parameters (12 core parameters as listed in their Table 2 under "Parameters that do not change with conditions") were held constant across all groups and conditions. These fixed parameters governed fundamental aspects of the model, such as the decision criterion (Φ), response boundaries (ΘT​,ΘF​), time per random walk step (χ), glitch probability (ξ) and duration (τξ​), overall LTM activation scaling (π), STM trace quality decay parameters (α,β), and STM similarity for matching traces and Stage 1 mismatching traces. The authors argue that, given the vastness and heterogeneity of the dataset, this degree of parameter consistency reflects a relatively low level of model complexity and supports the generality of the core mechanisms proposed by SLR23.

#### Estimated Parameter Values

A critical aspect of model assessment involves examining whether the estimated parameter values are not only statistically optimal but also conceptually reasonable and consistent with theoretical expectations. Lai, Cao, and Shiffrin (2024, p. 20-21) undertake such an examination for several key parameters of SLR23:

- **Short-Term Decay (**α,β**)**: The rate of decrease in STM trace quality, β, was estimated at approximately 1.70. This value, in conjunction with the asymptotic quality α (estimated at 0.21), produced the characteristic steep recency curves (target lag effects) observed in the data. While β directly reflects the rapid fall-off in performance with increasing lag, the interpretation of α is less direct, potentially representing a floor effect in STM quality at longer lags or some baseline contribution from more stable (perhaps LTM-like) representations of recent items.
    
- **Learning in CM Conditions (**Λj​**)**: The estimated learning rates systematically varied in line with a priori predictions.
    
    - Learning was stronger in the standard CM paradigm of Nosofsky et al. (2021) (e.g., CM-PUB Λ=1.23) compared to the new, more complex CM paradigm requiring side-contingent key learning (e.g., CM-E1 Λ=0.65, CM-E2 Λ=0.44). This difference is attributed to the simpler and potentially more direct target/foil category learning possible in the standard paradigm, in addition to response key learning.
        
    - Within each study type, learning rates were higher for pure CM conditions than for mixed conditions where CM items were presented less frequently, thereby offering fewer learning opportunities (e.g., CM-PUB Λ=1.23 vs. MIX1-PUB Λ=0.14; CM-E1 Λ=0.65 vs. MIX-E1 Λ=0.33).
        
    - There was also a subtle indication that learning might be faster in mixed conditions with fewer item types (e.g., MIX4 (CM, AN) Λ=0.09 vs. MIX8 (CM, AN) Λ=0.03 in Experiment 2 of the new studies), though this effect was small. This pattern of learning parameters lends credence to the model's conceptualization of how learning accumulates and influences decisions.
        
- **Similarity Differences (**Ω **values)**: The various similarity parameters also aligned with expectations.
    
    - Crucially, similarity for matching probes and traces was consistently higher than for mismatching ones, which is essential for the model to achieve above-chance recognition. For instance, in Stage 1, matching similarity (Ω1(match)​) was 9.4, while mismatching similarity (Ω1X(mismatch)​) was 1.3. In Stage 2, matching similarity (Ω2(match)​) was 4.1, whereas the six Ω2XX​ values for mismatching traces ranged from 0.03 to 0.13.
        
    - The variation among the six Ω2XX​ values, reflecting different pairings of AN, VM, and CM items in probe-trace mismatches, was considered a critical factor in the model's ability to capture the nuanced data patterns across all conditions. Lai, Cao, and Shiffrin (2024, p. 19, 22) justify this by appealing to the idea that task experience and attentional allocation can lead to differential encoding of AN, VM, and CM stimuli, thereby altering their effective similarity when they are inappropriately retrieved or compared.
        
- **Glitches (**ξ,τξ​**)**: The probability of a glitch (ξ) was estimated to be low (0.04), with a glitch response time (τξ​) of 1399 ms. The authors note that while removing glitches and refitting led to a somewhat worse fit and less reasonable parameter values, the necessity of glitches for modeling medians (as opposed to full RT distributions) is a judgment call. Their inclusion is partly justified by the certainty that some lapses of attention occur and by the common use of such mechanisms in other research to improve model adequacy (Lai, Cao, & Shiffrin, 2024, p. 18, 21).
    
- **Base Times (**τ0​**)**: The 13 estimated base times for the different participant groups appeared generally reasonable. They tended to be longer for more difficult conditions, with the longest base time (291 ms) observed for the pure VM condition in the Nosofsky et al. (2021) study, which was arguably one of the most challenging. This ordering by task difficulty suggests that the base times were capturing systematic, albeit unmodeled, variations in overall processing load or response caution across groups and conditions (Lai, Cao, & Shiffrin, 2024, p. 21).
    
- **Other Parameters**: Parameters such as the decision criterion (Φ), response boundaries (ΘT​,ΘF​), time per step (χ), and LTM activation scaling (π) are more difficult to interpret in isolation due to their complex interactions within the EBRW framework. However, their estimated values (e.g., Φ=0.54, ΘT​=3.6, ΘF​=−1.5, χ=91 ms, π=0.52) collectively enabled the model to reproduce the observed trends. The authors also acknowledge that the parameter space likely contains multiple local minima with similar goodness-of-fit, but the ordinal properties of the key interpretable parameters generally remained stable (Lai, Cao, & Shiffrin, 2024, p. 17, 21).
    

Overall, the parameter estimates for SLR23 were largely consistent with theoretical expectations and provided sensible interpretations for the observed behavioral data, bolstering confidence in the model's underlying processes.

#### Comparison with Nosofsky et al. (2021) model (SLR21)

A significant motivation for the development of SLR23 was to overcome limitations associated with the earlier SLR21 model (Nosofsky et al., 2021). While SLR21 successfully provided good quantitative fits to a subset of the data (the five groups from their 2021 paper) and shared many core processes with SLR23 (e.g., learning of CM associations, familiarity-based decisions), its implementation suffered from issues of excessive flexibility and implausible parameter estimates that weakened its theoretical conclusions (Lai, Cao, & Shiffrin, 2024, p. 21-22).

Lai, Cao, and Shiffrin (2024) highlight several specific problems with SLR21:

1. **Excessive Flexibility**:
    
    - SLR21 employed different sets of parameter values to predict data from pure versus mixed conditions, with some of these differences being substantial and lacking strong theoretical justification.
        
    - The model required a large number of parameters (16 for mixed conditions, 19 for pure conditions).
        
    - It incorporated special, ad-hoc parameters like 'boost' for mixed conditions and 'primacy' for pure conditions.
        
    - AN items were assigned four different short-term parameters in mixed conditions but not in pure conditions.
        
    - Critically, the LTM components (LTMOld​ and LTMNew​) in its core equation were allowed to differ for AN, VM, and CM items, effectively permitting somewhat independent fitting of different data subsets rather than a unified account.
        
2. **Implausible Parameter Values**: The parameter values required by SLR21 to achieve its fits were often counterintuitive:
    
    - In the mixed condition, the learning rate for NEW (foil) CM items was estimated to be three times larger than the combined learning and familiarity strength for OLD (target) CM items, despite OLD CM items being seen more frequently and expected to be better learned and more familiar.
        
    - A similar issue arose in the pure CM condition, which required a very large learning strength for NEW test items and a surprisingly small combined strength for OLD test items.
        
    - In the mixed conditions, SLR21 posited equal contributions of long-term traces for VM and AN items. This is problematic because AN items, by definition, have no matching long-term traces from prior trials, whereas VM items have many.
        

SLR23 was designed to rectify these shortcomings. As discussed, it applied the same core equations and, for the most part, the same parameter values to predict the entire dataset, encompassing both the Nosofsky et al. (2021) results and the new findings from Lai, Cao, and Shiffrin (2024). The limited parameter variations allowed in SLR23 were argued to be more principled and constrained. Furthermore, the estimated parameter values in SLR23 were shown to be more consistent with theoretical expectations (e.g., learning rates reflecting opportunities for learning, stronger familiarity for more exposed items).

Several key differences in the architecture of SLR23 are credited with its improved performance and theoretical coherence compared to SLR21:

- **Stages of Processing**: The introduction of a two-stage evidence accumulation process for familiarity was a crucial innovation. Lai, Cao, and Shiffrin (2024, p. 22) report that attempts to simplify SLR23 by removing the initial stage led to significant problems in fitting the data and maintaining plausible parameter values. This staged approach allows the model to differentiate between initial, perhaps more perceptually driven, memory access and subsequent, more elaborated retrieval that integrates various sources of information (STM, LTM, knowledge).
    
- **Refined Similarity Assumptions**: While SLR23 still allowed some similarity parameters (Ω2XX​) to vary with item type, this was a more constrained and theoretically motivated form of flexibility than that employed in SLR21. The assumption is that the _similarity of mismatching items in STM during Stage 2_ can be affected by the specific combination of AN, VM, or CM items being compared, reflecting differential encoding or attentional strategies. This is distinct from SLR21's broader freedom to adjust LTM parameters differently for AN, VM, and CM conditions. Lai, Cao, and Shiffrin (2024, p. 22) argue that these specific similarity variations were a minimal set required for an acceptable model, drawing parallels to the importance of stimulus-dependent similarity in exemplar models of categorization.
    
- **Glitches**: Although considered less critical than staged processing, the inclusion of a glitch mechanism provided a principled way to account for a small proportion of non-model-based responses, potentially improving overall fit and parameter estimation stability.
    

In summary, Lai, Cao, and Shiffrin (2024) position SLR23 as a significant advancement over SLR21. It provides a more unified, parsimonious, and conceptually sound account of a very extensive and complex dataset. By resolving the problematic issues of flexibility and parameter plausibility that affected SLR21, SLR23 offers a more robust "proof by example" that the multifaceted processes of short-term memory, long-term event memory, and long-term knowledge can be integrated into a coherent framework to explain recognition memory decisions across a wide array of conditions. The success of SLR23, achieved with largely unchanging and justifiable parameter values, lends substantial support to the core cognitive processes it embodies.