# Method

## Participants

Participants were recruited through Prolific Academic (prolific.com), an online research platform. Eligibility requirements included: (1) residence in the United States, (2) minimum age of 18 years, (3) English fluency, and (4) a minimum 98% approval rating on previous Prolific studies to ensure data quality. All procedures were approved by the Institutional Review Board of Indiana University (Protocol #18431). Participants provided informed consent before beginning the experiment.

Compensation was provided at an hourly rate of $10.50, with an estimated completion time of approximately 35 minutes (actual payment of approximately $6.13). Payment was contingent on achieving at least 60% overall accuracy on recognition trials to ensure participant engagement.

## Materials and Apparatus

### Stimuli

The experiment used 2,361 unique color photographs of everyday objects and scenes selected from a stimulus set of 2,400 images collected by Brady et al. (2008) from Talia Konkle's laboratory. Images were randomly allocated to different experimental roles to ensure sufficient non-overlapping stimulus pools for study items, initial test foils, and final test foils.

### Software and Technical Environment

The experiment was implemented using jsPsych version 7.3.3 (de Leeuw, 2015), a JavaScript library for web-based behavioral experiments providing millisecond-level timing precision. The experiment was hosted on the Cognition.run platform. Technical requirements included desktop computer access and Google Chrome browser usage, which were verified programmatically. Participants were required to use full-screen mode throughout the experiment to minimize distractions. All stimulus images were preloaded before testing to ensure smooth presentation timing.

## Design and Procedure

The experiment employed a within-subjects design with two main phases: (1) initial study-test cycles and (2) final recognition test, with a between-subjects manipulation of final test presentation order.

### Phase 1: Initial Study-Test Cycles

Participants completed 10 study-test cycles. Each cycle followed an identical three-stage structure:

**Study Phase**: Twenty unique images were presented sequentially for 2000 ms each with 100 ms inter-stimulus intervals, preceded by a 1000 ms fixation cross.

**Distractor Phase**: To prevent rehearsal, participants completed an arithmetic task. Following a 3000 ms prompt ("Summing up the digits as they appear"), eight randomly selected digits (4-9) were presented sequentially for 2000 ms each with 1000 ms intervals. Participants then entered the sum using number keys and received immediate feedback ("CORRECT!" or "INCORRECT!") displayed for 2000 ms.

**Recognition Phase**: A 20-item recognition test followed, consisting of 10 targets (randomly selected from the 20 studied images) and 10 foils (novel images). Each test image was presented for up to 3500 ms. Participants pressed 'J' for "old" (previously studied) and 'F' for "new" (not studied) items.

**Initial Test Feedback**: Crucially, participants received continuous performance feedback throughout the recognition phase. Below the response key reminder, running accuracy was displayed and updated after each response within the current list (e.g., "Your accumulated accuracy for this trial is: 75%"). This feedback was designed to maintain motivation and engagement across all 10 study-test cycles.

**Quality Control**: Responses faster than 150 ms triggered a "Too fast!" warning, while failures to respond within the time limit prompted a "You need to respond faster!" message. Both warning messages were displayed for 1500 ms to ensure participant attention without disrupting experimental flow.

This procedure created three distinct item types for each study list: (1) **Studied-and-Tested (S&T)**: items both studied and subsequently tested as targets, (2) **Studied-Only (SO)**: items studied but not selected as test targets, and (3) **Test-Only (TO)**: items that appeared only as foils during recognition testing.

### Phase 2: Final Recognition Test

Following the 10th study-test cycle, participants received new instructions for a final recognition test. They were instructed to respond 'J' for any image seen at any point during the experiment (whether studied or tested) and 'F' only for completely novel images.

**Test Composition**: The final test comprised 420 trials: 210 "old" items drawn from previous phases and 210 novel foils. From each of the 10 study lists, 7 items were randomly sampled from each of the three item types (S&T, SO, TO), yielding 210 old items total (10 lists × 3 item types × 7 items = 210). These were randomly intermixed with 210 completely novel foils.

**Final Test Procedure**: Each image was presented for up to 4000 ms using the same 'J'/'F' response mapping. The same response time warnings remained active.

**Final Test Feedback**: Unlike the initial test phase, running accuracy feedback was absent. Instead, participants received **provenance feedback** after each response: the test image reappeared for 500 ms with overlaid text indicating its experimental history (e.g., "This item had been studied or tested on list 7" for old items, or "This item had never been seen previously" for novel foils). This informative feedback was designed to maintain engagement during the lengthy final test.

**Between-Subjects Presentation Order**: Participants were randomly assigned to one of three final test conditions that determined the presentation sequence of the 420 trials. Participants received explicit instructions about their assigned condition:

- **Forward Condition**: Items were presented in blocks corresponding to their original study list order (List 1 items first, then List 2, continuing through List 10). Within each list block, the selected items (7 S&T, 7 SO, 7 TO items plus corresponding foils) were presented in random order.
    
- **Backward Condition**: Items were presented in reverse list order (List 10 items first, then List 9, continuing through List 1). Within each list block, items were randomly ordered.
    
- **Random Condition**: All 420 trials were completely randomized across lists, with no blocking by original study list.
    

The instructions informed participants which presentation format they would encounter, emphasizing that they should respond based on whether they had seen each image anywhere during the initial test phase, regardless of presentation order.

## Dependent Measures

### Initial Test Performance

- **Recognition accuracy**: Hit rates for target items and correct rejection rates for foil items within each of the 10 study-test cycles
- **Response times**: Latencies for recognition judgments
- **Distractor task performance**: Accuracy on arithmetic sum calculations as a measure of continued engagement
- **Serial position effects**: Performance patterns based on study and test positions within each cycle

### Final Test Performance

- **Overall recognition accuracy**: Performance across all 420 trials, analyzed by item type (S&T, SO, TO) and foil type
- **Memory differentiation**: Comparative recognition performance for items with different learning histories
- **Presentation order effects**: Impact of Forward, Backward, and Random conditions on recognition accuracy and response times
- **Long-term retention patterns**: Memory performance as a function of original study list position and temporal delay

### Compliance Measures

Response time warning frequencies and overall accuracy relative to the 60% performance criterion provided indices of participant attention and task compliance.