### **3.3. Data Analysis Strategy**

The primary objective of the data analysis is to conduct a systematic and comprehensive examination of the behavioral data from Experiment 1. The analytical strategy is designed to rigorously test the a-priori hypotheses outlined in Section 3.1 and to fully characterize the complex patterns of recognition memory performance that emerge across the experiment’s distinct phases. This involves a multi-layered approach, beginning with data preprocessing and cleaning, followed by detailed analyses of the initial study-test phase and the final cumulative test phase. The outcomes of these analyses will serve two critical purposes: first, to provide a clear empirical description of within-list, between-list, and long-term context effects; and second, to establish a set of precise, quantitative benchmarks that will be used to constrain and validate the computational model developed in Chapter 5. All data processing and statistical analyses will be conducted using the R programming language and environment (Version 4.3.2; R Core Team, 2023), leveraging key packages from the `tidyverse` suite (e.g., `dplyr`, `ggplot2`) for data manipulation and visualization, and the `lme4` package for fitting mixed-effects models.

**Data Preprocessing and Exclusion Criteria**

Prior to formal analysis, the raw data collected from the jsPsych script will undergo a rigorous preprocessing and cleaning pipeline to ensure the integrity and validity of the results. This pipeline involves several sequential steps. First, participant-level exclusions will be applied. Any participant who did not complete the entire experimental session will be removed from the dataset. Furthermore, in line with the performance-contingent compensation model described in the methodology, any participant failing to achieve the pre-specified minimum overall accuracy of 60% in the final test will be excluded. This criterion serves as a critical quality control measure, filtering out individuals who were likely inattentive, failed to understand the instructions, or responded randomly, thereby ensuring that the final dataset reflects engaged and compliant participation.

Following participant-level exclusions, trial-level cleaning will be performed. Given the time limits imposed during the recognition tests, trials with no response will be coded as incorrect. Responses with exceptionally fast or slow latencies are often indicative of anticipatory key presses or lapses in attention, respectively. Therefore, individual recognition trials with response times (RTs) falling outside a plausible range will be excluded from all RT analyses, though they will be retained and scored for accuracy analyses. Based on the procedural parameters and typical response distributions in recognition tasks, any trial with an RT below 200 ms or above 3,500 ms will be flagged and removed from latency-based calculations.

**Analysis of the Initial Study-Test Phase**

The analysis of the ten initial blocks is designed to investigate how memory performance evolves at both a local (within-list) and a global (between-list) scale. The primary dependent variables for this phase are recognition accuracy—decomposed into Hit Rate (H) for target items and Correct Rejection (CR) rate for foil items—and response times for correct decisions.

1. **Within-List Effects:** To understand the influence of an item's position within a given learning episode, we will analyze performance as a function of both study and test position.
    
    - **Study Position (Serial Position Curve):** To assess primacy and recency effects, the Hit Rate for target items will be calculated and plotted as a function of their original serial position (1–20) within the study list. This will be averaged across all ten lists to create a comprehensive within-list serial position curve.
    - **Test Position (Output Interference):** To investigate whether the act of retrieval affects subsequent memory judgments, both Hit Rates and Correct Rejection rates will be analyzed as a function of their serial position (1–20) within the test block. A decline in performance across test trials would be indicative of output interference.
2. **Between-List Effects (Proactive Interference):** To measure the cumulative effect of learning on memory, we will examine how performance changes across the ten sequential lists. The mean Hit Rate and Correct Rejection rate will be calculated for each list (Block 2 through Block 11) and plotted as a function of list number. A systematic decline in performance, particularly in Hit Rate, would provide strong evidence for the build-up of proactive interference as more items are encoded into memory.
    

**Statistical Modeling for the Initial Phase:** To formally test these effects, we will employ Generalized Linear Mixed-Effects Models (GLMMs) with a binomial link function, which are ideally suited for analyzing trial-level accuracy data while accounting for the nested structure of the design. A comprehensive model will be constructed to predict the probability of a correct response based on fixed effects for Item Type (target vs. foil), List Number (as a continuous or factor variable), Study Position (for targets), Test Position, and all relevant two-way and three-way interactions. To account for non-independence of observations, the model will include random intercepts for both `participants` and `stimulus items`. This approach allows us to generalize the findings beyond the specific sample of participants and stimuli used, providing more robust inferences about the underlying cognitive processes.

**Analysis of the Final Test Phase**

The analysis of the surprise final test is central to the experiment's aims, designed to provide a deep probe of long-term memory representations and the influence of contextual reinstatement. The dependent variables are the Hit Rates for the different categories of "old" items and the False Alarm (FA) rate to the truly novel foils.

1. **Item History Effects:** The core analysis will be a comparison of recognition performance for items with different learning histories. We will calculate and compare the Hit Rates for the three critical item types:
    
    - **Studied-and-Tested (S&T):** Items that were both studied and successfully retrieved in the initial phase.
    - **Studied-Only (SO):** Items that were studied but not tested initially.
    - **Previously-Tested Foils (PTF):** Items initially encountered as new foils during the initial tests. Comparing H(S&T) to H(SO) will provide a direct measure of the long-term memorial benefit of testing (the testing effect). Comparing H(SO) to H(PTF) will elucidate the difference between intentional encoding and incidental exposure.
2. **Long-Term Serial Position and Retrieval Order Effects:** We will investigate how memory for items is affected by their original temporal context and whether explicit cues can modulate retrieval.
    
    - **Long-Term Primacy and Recency:** Performance (Hit Rate) will be plotted as a function of the items' original list number (1–10). This will reveal the shape of the long-term retention function and assess the persistence of primacy and recency effects over a long delay filled with significant interference.
    - **Influence of Retrieval Order:** We will conduct a between-subjects comparison of the performance patterns in the **Forward**, **Backward**, and **Random** final test conditions. Specific interest lies in whether the Forward and Backward conditions, which provide explicit contextual cues about the upcoming items' list of origin, lead to different performance profiles compared to the unstructured Random condition. We will examine interactions between Retrieval Order and Original List Number to see if, for example, the Forward condition preferentially boosts memory for early-list items.
3. **Final Test Output Interference:** To assess output interference in the final test, recognition accuracy for all item types will be analyzed as a function of their position within the 420-trial final test sequence. Performance will be binned (e.g., into 10 blocks of 42 trials) and plotted to visualize performance degradation over the course of the lengthy test.
    

**Statistical Modeling for the Final Phase:** As with the initial phase, GLMMs will be the primary tool for statistical inference. A global model will be constructed to predict trial-level accuracy on "old" items. The model will include fixed effects for Item History (S&T, SO, PTF), Original List Number (continuous), Final Test Position (continuous), and the between-subjects factor of Retrieval Order (Forward, Backward, Random). Critically, we will test for all meaningful interactions between these factors. The random effects structure will again include random intercepts for `participants` and `stimulus items`.

This comprehensive, model-driven analysis strategy is designed to extract maximal information from the rich dataset generated by Experiment 1. By systematically dissecting performance at multiple timescales and across different experimental manipulations, this approach will provide a detailed empirical foundation for the subsequent chapters, enabling a robust evaluation of contemporary theories of context and memory and providing stringent targets for the computational modeling efforts that form the core of this dissertation.