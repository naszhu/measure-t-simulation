## **Chapter 5: Computational Modeling of Recognition Experiments 1 & 2 with an Extended REM Framework**

### **5.1. Introduction to the Modeling Approach**

The preceding chapters have detailed the empirical findings from two picture recognition experiments designed to investigate the multifaceted role of context in episodic memory. Experiment 1 (Chapter 3) explored contextual influences using novel foils, revealing distinct patterns of within-list and between-list performance, as well as the impact of retrieval conditions in a final cumulative test. Experiment 2 (Chapter 4) extended these investigations by introducing mixed foil types with varying prior episodic histories, thereby creating a more complex and challenging recognition environment. These experiments have provided a rich dataset highlighting how participants encode, update, and utilize contextual information to make recognition decisions, particularly in multi-list paradigms where context dynamically evolves and interference from prior experiences is prevalent. While these behavioral data offer valuable insights, a deeper, mechanistic understanding of the underlying cognitive processes necessitates the development and application of a formal computational model. This chapter introduces the computational modeling approach undertaken in this dissertation, which aims to provide a principled and quantitative account of the observed context effects within an extended version of the Retrieving Effectively from Memory (REM) framework (Shiffrin & Steyvers, 1997).

The REM model, in its original formulation, has proven to be a powerful and influential framework for understanding episodic memory, particularly recognition memory. Its core tenets include the storage of separate episodic images (traces) for different experienced items, where each image is represented as a vector of feature values. These features can represent various aspects of an item, including its content and the context in which it was encountered. The storage process is probabilistic and imperfect; episodic traces are often incomplete and error-prone copies of the studied information. During retrieval, a test probe (also a vector of feature values) is matched in parallel to the stored episodic images. The critical computation involves calculating a likelihood ratio for each image, assessing the probability that the observed match between the probe and the image would have occurred if the image was a trace of the probe item (an s-image) versus if it was a trace of a different item (a d-image). These likelihood ratios are then combined (typically averaged) to determine the overall odds that the test item is "old," which forms the basis for the recognition decision. REM has successfully accounted for a range of fundamental recognition phenomena, including list-length effects, strength effects, list-strength effects, and aspects of word frequency effects (Shiffrin & Steyvers, 1997). Its explicit feature-based representation and its principled Bayesian decision process make it a highly suitable foundation for investigating the nuanced role of context.

However, the standard REM model, while robust, requires extensions to fully capture the complexities of contextual dynamics observed in Experiments 1 and 2. The multi-list design, the deliberate manipulation of foil types with specific prior exposures, the potential for memory traces to be updated during test phases, and the evolving nature of context both between and within experimental lists, all point to the need for a more elaborated framework. For instance, standard REM does not inherently specify how different types of context (e.g., global, stable context versus local, fluctuating context) are represented, nor does it fully detail mechanisms for context drift over time, context reinstatement, or the strategic use of context in filtering memory search. The experiments presented in this dissertation were specifically designed to probe these more dynamic aspects of context, thus necessitating an extension of the REM architecture to incorporate mechanisms that can address these phenomena directly.

The primary goals of the computational modeling effort in this dissertation are therefore multifaceted and directly tied to the empirical findings:

1. **To simulate and account for the key behavioral patterns observed in Experiment 1 (novel foils):** This includes modeling performance in the initial study-test phases, such as within-list effects (e.g., serial position effects for study and test items) and between-list effects (e.g., changes in accuracy or response bias across the 10 lists). Furthermore, the model aims to capture performance in the final, cumulative test phase, including how different retrieval orders (random, forward, backward) might interact with the retrieved contextual information.
    
2. **To extend the modeling to Experiment 2 (mixed foils):** A crucial goal is to demonstrate the model's capacity to handle the increased mnemonic challenge posed by foils with varying prior episodic histories. This involves accounting for the differential false alarm rates to these foil types and capturing any systematic differences in hit rates or overall accuracy compared to Experiment 1. The model should be able to explain how the memory system differentiates targets from lures that share varying degrees of familiarity or prior contextual associations.
    
3. **To use the model to explore the theoretical implications of specific contextual components and mechanisms:** Beyond fitting the data, the model serves as a tool for understanding the functional significance of its architectural assumptions. This includes investigating how the interplay between different types of context (e.g., changing vs. unchanging context features), mechanisms of context change (e.g., inter-list shifts, intra-list drift, study-to-test drift), context reinstatement processes, and memory updating during testing collectively contribute to the observed memory performance.
    
4. **To provide a unified and principled account of context processing:** Ultimately, the aim is to offer a coherent theoretical framework that explains how context is represented, encoded, stored, dynamically updated through experience (including testing), and utilized to guide recognition decisions across the varied and complex experimental conditions explored in this dissertation.
    

To achieve these goals, an extended REM framework has been developed. This framework, while retaining the core principles of REM, incorporates several specific elaborations designed to address the dynamic nature of context. As will be detailed in Section 5.2, these extensions include: an explicit distinction between **Changing Context (CC)** features (which are subject to change between lists, governed by a probability δlist​) and **Unchanging Context (UC)** features (which remain stable across lists); mechanisms for **context drift** (affecting both CC and UC features between study and test phases within a list, governed by a probability δdrift​); and a process of **context reinstatement** (where the study context can be partially restored after each test, governed by a probability δreinstate​). The model also makes specific assumptions regarding feature encoding, including distinct geometric base rates for content and context features (gword​, gcontext​), a general probability of storage (U∗) which can vary for specific events like the first list or first test item, and a copying fidelity parameter (c). Crucially, the extended model incorporates **memory updating processes during testing**, allowing for the strengthening of content features for correctly identified old items and the formation of new traces for items encountered as foils. Finally, the decision process is conceptualized as a two-stage operation involving an initial **context filtering** step (whereby traces are activated if their context sufficiently matches the probe context, based on a context threshold τ) followed by an evaluation of **content features** against a decision criterion (Θj​) that can itself vary, for instance, across test positions or between initial and final tests.

In summary, this chapter embarks on the computational modeling phase of the dissertation. The approach is to leverage the foundational strengths of the REM model and extend its architecture with specific, theoretically motivated mechanisms to account for the rich patterns of context effects observed in Experiments 1 and 2. The following sections will describe the extended REM model in detail (Section 5.2), present its application to the data from Experiment 1 (Section 5.3) and Experiment 2 (Section 5.4), and discuss the broader implications of these modeling efforts (Section 5.5). It is anticipated that this comprehensive modeling endeavor will yield significant insights into the intricate ways context shapes our episodic memory.

### 5.2. The Extended REM Model Architecture

The computational model developed in this dissertation is an extension of the Retrieving Effectively from Memory (REM) framework. While it retains the fundamental principles of the original REM model, it incorporates several elaborations designed to address the dynamic nature of context as explored in Experiments 1 and 2. This section details the architecture of this extended model, outlining its core assumptions, the specific mechanisms for representing and updating context, and the processes governing the encoding and storage of memory traces.

#### 5.2.1. Core REM Principles

The foundation of the current model is the REM framework developed by Shiffrin and Steyvers (1997). REM is a probabilistic model of episodic memory that has successfully accounted for a range of classic recognition phenomena. Its central assumptions, which are carried forward into the extended model, provide a principled basis for understanding how memories are stored and retrieved.

Episodic Images as Feature Vectors: In REM, memories of individual events (e.g., studying a word or picture) are stored as separate episodic images or traces in memory. Each image is conceptualized as a vector of feature values. These features can represent any aspect of an experience, including the sensory details of an item (its content) and the environment or mental state in which it was encountered (its context). The value of a feature is represented by a positive integer, where different values have different environmental base rates; a value of zero indicates that no information about that feature was stored. For simplicity, REM assumes that the environmental base rates of feature values follow a geometric distribution, governed by a parameter, g. This means that lower integer values are more common, while higher values are rare and therefore more diagnostic when a match occurs in memory.

Probabilistic and Error-Prone Storage: The process of encoding a new memory trace is imperfect. REM assumes that storage is both incomplete and error-prone. When an item is studied, the model does not create a perfect replica of the item's feature vector in memory. Instead, for each feature of the studied item, there is a probability that a value will be stored. If a feature is selected for storage, its value is copied correctly from the study item with a certain probability, c (the copying fidelity parameter). With probability 1−c, an incorrect value is stored, chosen at random from the environmental base rate distribution. This mechanism captures the fallibility of human memory, where traces are often partial and may contain errors. Repetitions of an item are assumed to strengthen a single episodic image rather than creating new, separate ones.

Parallel Retrieval and Likelihood-Ratio Competition: Retrieval in REM is a parallel process where a test probe (also a feature vector) is matched against all stored episodic images simultaneously. The core of the retrieval process is the calculation of a likelihood ratio, λj​, for each stored image j. This ratio assesses the probability that the observed pattern of matching and mismatching feature values between the probe and the image would have occurred if the image were a trace of the probe item (an "s-image" for same) versus if it were a trace of a different item (a "d-image" for different). Features that match between the probe and a memory trace increase the likelihood ratio, especially if the matching feature value is rare (i.e., has a high integer value). Mismatching features decrease the ratio. This calculation provides a principled way to quantify the evidence that a particular memory trace provides for a test probe.

Bayesian Decision Process: The final recognition decision is based on a Bayesian calculation of the overall odds that the test item is "old" (i.e., corresponds to one of the stored images) versus "new". The model posits that the odds, Φ, are calculated by averaging the likelihood ratios (λj​) from all the memory traces under consideration: Φ=n1​∑j=1n​λj​, where n is the number of traces. This computed odds value is assumed to correspond to the subjective feeling of familiarity. By default, the model makes an "old" response if the odds are greater than 1.0 and a "new" response otherwise. This Bayesian foundation is what makes retrieval "effective" in REM; it provides a normative basis for the decision, given the available evidence and the inherent uncertainty of the stored traces. This core decision mechanism is powerful, as it allows the model to predict "mirror effects"—where a factor simultaneously increases the hit rate and decreases the false alarm rate—without needing to assume strategic shifts in the decision criterion.

#### 5.2.2. Context Representation

A primary goal of this dissertation is to investigate the dynamic role of context. The standard REM model, while allowing for context features, does not specify the mechanisms by which context evolves over time and influences retrieval in a multi-list paradigm. The extended model therefore incorporates a more detailed and structured representation of context and explicit mechanisms for its change, drift, and reinstatement. The entire feature vector for any given item is assumed to comprise 75 features in total. These are divided into content features and two distinct types of context features.

Changing vs. Unchanging Context: A key innovation of the extended model is the division of context features into two categories: Changing Context (CC) and Unchanging Context (UC).

    Unchanging Context (UC): This component consists of 25 features that are assumed to remain stable across all 10 experimental lists. UC features represent the static aspects of the experimental environment, such as the physical setting (or the digital interface of the online experiment), the general task instructions, and other elements that do not change from one list to the next.

Changing Context (CC): This component also consists of 25 features, but these are subject to change between lists. The model assumes that each of the 25 CC features has a probability, δlist​=0.14, of changing its value from one list to the next. These features are intended to capture the more transient and fluctuating aspects of context, such as a participant's thoughts, focus of attention, or physiological state, which naturally vary over the course of a longer experiment. This mechanism of inter-list context change is crucial for explaining between-list effects, where performance is influenced by the accumulation of, and interference from, prior list contexts.

Context Drift (Study-to-Test): Within a single list, context is not perfectly static. The mental state of a participant can shift between the study phase and the test phase. To capture this, the model incorporates a mechanism for intra-list context drift. After the study phase of a list is complete and before the test phase begins, both the CC and UC features are subject to a drift process. Each of the 50 context features has a probability, δdrift​=0.14, of changing its value. This drift helps to account for the distinction between study context and test context, a factor known to be important in memory performance. The distractor task placed between study and test in the experiments is the empirical analogue of this drift process, serving to create temporal and psychological distance between encoding and retrieval.

Context Reinstatement: Memory is not a passive process; retrieval events can influence subsequent memory states. The extended model includes a mechanism for context reinstatement, which posits that the act of testing can cause the participant's mental state to revert partially toward the context that was active during the preceding study phase. After each test trial within a list, the model assumes that the drifted context has a chance to be reinstated back to its pre-drift, study-phase state. This is governed by a probability δreinstate​=0.4. This reinstatement mechanism allows the model to capture how retrieval can refresh or reactivate prior contextual representations, potentially influencing performance on subsequent trials within the same test block. Over several lists, a reinstatement probability of 0.4 implies that a substantial portion of the original study context features would be restored.

These explicit mechanisms for representing a multi-faceted context and governing its temporal dynamics allow the extended REM framework to model the complex interplay of contextual factors that was specifically targeted by the designs of Experiments 1 and 2.
5.2.3. Feature Encoding and Storage

The process of storing information in memory is central to the model's ability to account for learning and forgetting. The extended model specifies the parameters that govern how item content and context features are encoded into episodic traces. This process involves distinct base rates for content and context, a general probability of storage that can vary for specific events, and a fixed parameter for copying fidelity.

Content and Context Feature Representation: The model assumes a total of 75 features for each stored event trace. These are broken down into:

    25 Content Features (C): These features are tied directly to the specific perceptual details of the studied pictures.

50 Context Features: As described above, these are further divided into 25 Changing Context (CC) features and 25 Unchanging Context (UC) features.

Geometric Base Rates: The diagnosticity of a feature is determined by its rareness, or base rate, in the environment. The model assigns separate geometric base rate parameters for content and context features.

    The base rate for content features (referred to as g_word in the model description, likely reflecting an earlier version using word stimuli) is set to gword​=0.4.

The base rate for context features is set lower, at gcontext​=0.3. A lower g value means that feature values, on average, will be higher (less common), making them more diagnostic if a match occurs. The distinction allows the model to treat the inherent statistical properties of item and context information differently.

Probability of Storage (U∗): Not all information from an event is successfully stored. The probability of storage, U∗, determines the likelihood that a feature from a study item will be encoded into a memory trace. The extended model introduces a critical dynamic to this parameter, making it sensitive to an item's position within the experiment. This is intended to capture primacy or attention effects, where the very first events in a new context are encoded more strongly.

    First List Primacy: For items studied in the very first list of the experiment, the storage probability U∗ is set to 0.08. For all subsequent lists (2 through 10), this base probability is halved to 0.04, reflecting a decrease in novelty or attention.

First Test-Item Primacy: A similar primacy effect is assumed for the first item encountered in each test phase. For the first test item of the first list, U∗ is given its highest value, 0.1. For the first test item in all subsequent lists, U∗ is 0.08. All other test items are stored with the base probability of 0.04.

This nuanced approach to storage probability allows the model to account for performance differences observed at the very beginning of the experiment and at the start of each new list, which are common findings in multi-list learning paradigms.

Copying Fidelity (c): When a feature is stored, it is not always stored perfectly. The copying parameter, c, represents the probability that the feature's value is copied correctly from the source item. The model sets this parameter to c=0.8 for most storage events. This means that 80% of the time a feature is stored, its value is accurate, while 20% of the time an error is made and a random value is stored instead. However, there is a special case: for items that are tested as targets, the model assumes perfect copying (c=1) during the memory updating process at test, reflecting the high-fidelity re-encoding of correctly recognized items. This ensures that memory traces for successfully retrieved items are robustly strengthened.

#### 5.2.4. Memory Update During Testing (Restorage)

A critical feature of human memory is that retrieval is not a passive act; the process of testing a memory can modify its underlying trace. This principle, often summarized as "testing is a learning event," is explicitly incorporated into the extended REM model through a process of memory updating, or "restorage," during the test phases. This mechanism allows the model to capture how experiences during recognition tests—both for successfully identified targets and for encountered foils—shape the long-term representation of memories. The model assumes different updating processes for items judged "old" versus those judged "new."

Strengthening of Old Item Traces: When a test item is presented and correctly identified as "old" (i.e., it is a target), the model assumes its corresponding memory trace is retrieved and strengthened. A key assumption in the extended model is that this strengthening process applies only to the content features of the trace, not the context features. The process unfolds as follows: the model first identifies the memory trace that provides the strongest evidence for the probe—the one with the maximum likelihood ratio. Then, for this trace, each content feature is compared to the corresponding feature in the test probe. Any content features in the trace that were previously blank (i.e., had a value of 0, indicating nothing was stored) or that mismatched the probe now have a probability of being restored. This restoration means the feature value from the test probe is copied into the memory trace, effectively filling in missing information and correcting errors.

This content-specific strengthening has important theoretical implications. It suggests that while the "what" of a memory (its content) is reinforced through successful retrieval, the "where" and "when" (its context) are not similarly updated. Furthermore, the model makes a strong assumption about the fidelity of this updating process for targets. When an item is tested as a target, its restorage is assumed to be perfect, with a storage probability of 1 and a copying fidelity of 1. This ensures that the memory traces for items that have been successfully tested become highly robust and accurate representations of the item's content, which in turn affects their potential to cause interference on subsequent trials.

New Trace Formation for New Items: When a test item is presented and judged (correctly or incorrectly) to be "new," the model assumes that a new episodic trace is formed for it in memory. This applies primarily to novel foils, which are encountered for the first time during the test phase. The formation of this new trace follows the same general storage principles as in the study phase. The foil item's content and context features are encoded into a new episodic image, governed by the same parameters for storage probability (U∗) and copying fidelity (c).

However, the model posits a crucial difference in the strength of traces formed for foils compared to those for targets. While a tested target is restored with perfect fidelity, a foil is stored with the standard, lower probability of storage (U∗=0.04) and copying fidelity (c=0.8). This differential storage strength is fundamental to the design of Experiment 2, where the history of an item—whether it was previously a target or a foil—determines its potential to act as a potent lure in subsequent lists. An item previously tested as a target will have a much stronger and more complete content representation in memory than an item that was only ever encountered as a foil, making it a more confusable distractor if its original context cannot be accurately retrieved. This memory updating mechanism is therefore essential for the model to simulate the differential false alarm rates for the various foil types explored in Experiment 2.

#### 5.2.5. Decision Process

The retrieval and decision process in the extended REM model is conceptualized as a two-stage operation that leverages the detailed context and content representations. This sequential process allows the model to first narrow down the search space using contextual cues and then make a fine-grained recognition judgment based on item-specific content. This architecture is a departure from the single-step odds calculation in the original REM model and is designed to handle the high levels of interference present in multi-list experiments with many stored traces.

Stage 1: Context Filtering: The first step in the decision process is context filtering. When a probe item is presented at test, the model first uses only the context features (both Changing and Unchanging) to perform an initial match against all the traces stored in memory. A likelihood ratio is computed for each trace based solely on the match between the probe's context vector and the trace's context vector. This likelihood value is then compared against a high context threshold, τ=100.

Only those memory traces whose context-based likelihood ratio exceeds this threshold are passed on to the next stage. This initial filtering step serves as a powerful mechanism for managing interference. Because context changes between lists (due to δlist​) and drifts over time, traces from the current list are most likely to have a context representation that closely matches the probe's context. Traces from remote prior lists will have significantly different context vectors and are therefore unlikely to surpass the high threshold. This step effectively uses global context as a cue to activate a relevant subset of traces—primarily those from the most recent list—thereby functionally isolating the current list's memory search from the vast number of irrelevant traces accumulated from prior experiences. This mechanism is directly analogous to the theoretical concept of a "retrieval set" and is critical for the model's ability to handle the thousands of stored images in a computationally tractable way.

Stage 2: Content Feature Evaluation: Once the set of candidate traces has been activated via context filtering, the second stage of the decision process begins. In this stage, the model evaluates the content features of the probe against the content features of only the activated traces. For this restricted set of traces, content-based likelihoods are calculated. These likelihoods are then combined (averaged) to produce the final odds value, which represents the evidence that the probe item is "old" within the contextually relevant set.

This final odds value is then compared against a decision criterion, Θj​. A crucial feature of the extended model is that this decision criterion is not static. It is assumed to change dynamically across the test positions within a list, reflecting shifts in participant strategy or response bias as a test block progresses. Specifically, the criterion Θj​ is assumed to decrease linearly from an initial value of 1 down to 0.6 across the 20 test positions (j) in a given list. A decreasing criterion means that the model becomes more liberal in its willingness to endorse an item as "old" later in the test list, a dynamic that can help account for patterns of output interference or criterion shifts observed in empirical data.

Furthermore, the model assumes a different criterion setting for the final, cumulative test phase. In the final test, the criterion for content evaluation is set to a strict, constant value of 1. This reflects the increased difficulty and different demands of the final test, where participants must discriminate items seen throughout the entire experiment from new foils, likely adopting a more conservative response strategy to avoid false alarms to the large number of familiar-seeming lures. This two-stage decision process, with its dynamic criteria, provides the model with the flexibility needed to capture the complex behavioral patterns observed in both the initial list-by-list tests and the final comprehensive test.

5.2.6. Model Parameters Summary

The extended REM model's architecture is defined by a set of parameters that govern context dynamics, feature encoding, memory updating, and the decision process. These parameters were chosen to provide a quantitative instantiation of the theoretical principles guiding the model and to enable simulations of the behavioral data from Experiments 1 and 2. The table below provides a comprehensive summary of these key parameters, their roles within the model, and their specified values as used in the simulations.

