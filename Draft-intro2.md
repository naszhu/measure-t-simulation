

# Core processes in simple REM
The simple version of REM (Shiffrin & Steyvers, 1997) specifies a small set of processes that have come to be regarded as fundamental to recognition memory. These processes define both the representational assumptions of the model—how traces are encoded and stored—and the decision mechanisms by which recognition judgments are made.
## **Noisy storage of features.**  
A central process in REM is the assumption that episodic traces are stored noisily. When an item is studied, only a subset of its features is encoded, and those that are stored may be inaccurate or corrupted (Shiffrin & Steyvers, 1997). Each trace is therefore a probabilistic, degraded record rather than a straight copy. This principle has a long lineage: Formal models like SAM (Gillund & Shiffrin, 1984; Raaijmakers & Shiffrin, 1981) and MINERVA (Hintzman, 1984) already treated episodic encoding as partial and error-prone. REM formalized this idea mathematically, specifying that each feature is stored with probability less than one and may be replaced by an incorrect value drawn from environmental base-rate distributions. As a result, repeated presentations of the same item produce overlapping but non-identical traces, ensuring that memory contains variability rather than perfect replicas. This stochastic encoding is essential for recognition because it creates the variability across traces that produces overlapping familiarity distributions for studied and novel items. In this way, noisy storage provides the representational variability on which REM’s later evidence-weighting and decision processes are built.
## **Diagnosticity-Weighted Global Matching in REM**  

A second component of this first cluster of processes in REM is the treatment of diagnosticity. Earlier global-similarity models such as SAM (Gillund & Shiffrin, 1984) and MINERVA (Hintzman, 1984, 1986) assumed that all features contributed equally to recognition decisions. REM instead assigns evidential weight to features according to their informativeness, distinguishing two complementary sources of diagnosticity. At the environmental level, features differ in their base rates: common features are less informative, while rare features are highly diagnostic. By assuming that feature values are drawn from a geometric distribution, REM formalizes this idea in a principled Bayesian manner, so that matches on rare features yield larger likelihood ratios than matches on common ones. This weighting was a crucial innovation, providing the mechanism by which REM explains robust phenomena such as the mirror effect, where low-frequency words and pictures simultaneously produce higher hit rates and lower false alarms.

In addition to environmental base rates, diagnosticity also depends on the similarity structure of the particular experimental set. When items within a list share many overlapping attributes, only a subset of features will distinguish among them, and these become especially diagnostic in the task context. Conversely, when items are highly heterogeneous, even moderately common features may retain discriminative value. REM therefore incorporates both environmental and task diagnosticity into its evidence calculations. This makes recognition judgments responsive to both the long-term statistics of the environment and the distinctive structure of the current study set.

## Parallel Probe–Trace Comparison in REM

Another central process in REM is the parallel comparison of the probe to all stored traces. This principle has its roots in the “global matching” tradition of memory models developed in the 1970s and 1980s, where recognition was assumed to reflect the combined influence of all traces rather than the retrieval of a single best match (e.g., Gillund & Shiffrin, 1984; Hintzman, 1984, 1986). Global comparison was introduced to explain why memory judgments are graded and probabilistic, and why performance is sensitive to list composition: a probe is influenced not only by its own trace but also by similarity to other stored items. REM adopted this assumption but reformulated it within a likelihood-ratio framework, specifying that every trace contributes evidence according to the probability that its features could have generated the probe.

The importance of this parallelism is twofold. First, it grounds recognition in the full distribution of memory, allowing interference and variability across episodes to systematically affect recognition outcomes (e.g., Clark & Gronlund, 1996). Second, by computing likelihood ratios for each trace and then integrating them, REM translates the intuitive idea of global similarity into a principled Bayesian evaluation of the memory store (Shiffrin & Steyvers, 1997). In this way, the parallel comparison mechanism ensures that recognition decisions reflect the combined influence of all stored traces, capturing interference and variability across episodes (Mohamed, Meyer-Grant, & Shiffrin, in press).
## **Familiarity as the average likelihood ratio.**  
REM defines familiarity as the decision variable that follows from the parallel comparison process. After the probe is compared to all traces, the resulting likelihood ratios are averaged to compute a global “odds” value, which serves as the measure of familiarity (Shiffrin & Steyvers, 1997). A decision criterion is applied to this average: if the odds exceed 1.0, the item is judged _old_; otherwise, it is judged _new_. This Bayesian derivation makes the decision process optimal in the sense that it uses all the diagnostic information available in memory traces for distinguishing targets from foils.

The importance of defining familiarity in this way is twofold. First, it embeds recognition within a normative likelihood-based framework, offering a principled account of why familiarity judgments track discriminability so well across conditions such as list length and word frequency (Shiffrin & Steyvers, 1997, 1998). Second, it links REM to broader recognition paradigms traditionally described with signal detection theory. Whereas signal detection theory treats familiarity as a latent strength sampled from overlapping distributions (Green & Swets, 1966; Wixted, 2020), REM grounds the familiarity signal in concrete feature–trace comparisons, showing how the distributions arise mechanistically from storage noise and feature diagnosticity. This mechanistic account allows REM to generalize across formats, including old–new tests, two-alternative forced choice, and more complex multi-item judgments (Mohamed, Meyer-Grant, & Shiffrin, in press).

Critically, this conception of familiarity as an average likelihood ratio explains why recognition can succeed without explicit recall of episodic details—a phenomenon long noted in remember–know paradigms (Mandler, 1980; Yonelinas, 2002). Even when no trace is directly retrieved, the accumulated similarity signal provides a robust probabilistic basis for decision. In this sense, REM formalizes a process that earlier models such as SAM (Gillund & Shiffrin, 1984) only described heuristically, and demonstrates that a familiarity-only decision rule can capture the major qualitative patterns of recognition memory performance.
