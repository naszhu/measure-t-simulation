
## difference between kernel and marginal distribution

| **Concept**               | **Definition**                                                                                                                                   | **Example**                                                   |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------- |
| **Kernel**                | A function $k: \mathcal{A} \times \mathscr{B} \to [0,1]$ where:  <br>1. $k(a,\cdot)$ is a probability measure  <br>2. $k(\cdot,B)$ is measurable | $k(\theta, A) = P(X \in A \mid \theta)$ (Bayesian likelihood) |
| **Marginal Distribution** | Obtained by integrating/summing over other variables: $P_X(A) = \int P_{X,Y}(A \times dy)$                                                       | $P_X([160,170]) = \sum_{y} P_{X,Y}([160,170], y)$             |

- Given (Î˜, B_Î˜) and (ğ’³, B_ğ’³), a kernel k: Î˜ Ã— B_ğ’³ â†’ [0,1] satisfies:
    
    1. For fixed Î¸ $\in\Theta$ , k(Î¸, Â·) is a probability measure on ğ’³
        
    2. For fixed A âˆˆ B_ğ’³, k(Â·, A) is measurable
        
- **Bayesian likelihood**: Kernel from Î˜ to ğ’³
    
    - k(Î¸, A) = P_X(X âˆˆ A | Î¸)
        
- **Frequentist likelihood**: Function L(Î¸; x),Â **not**Â a kernel (Î¸ is fixed)

### Â Bayesian Inference: Kernels vs. Joint Distributions
**Kernel perspective**Â (more general):

- Prior: $P_\theta$ (measure on $\Theta$)
    
- Likelihood: Kernel $k: \Theta \times \mathscr{B}_{\mathcal{X}} \to [0,1]$
    
- Posterior: Kernel $q: \mathcal{X} \times \mathscr{B}_{\Theta} \to [0,1]$

(what is given is write at first as the space, and the sigma algebra is for unknown)

**Joint distribution perspective**:

- Define joint measure on $\Theta \times \mathcal{X}$:  
    $P_{Î¸,X}(CÃ—A)=âˆ«_Ck(Î¸,A)dP_Î¸(Î¸)$
    
- Posterior obtained via conditioning