---
bibliography: references/references.bib
---

### **5.2. The Extended REM Model Architecture**

The computational model developed to account for the findings of Experiments 1 and 2 is an extension of the Retrieving Effectively from Memory (REM) framework (Shiffrin & Steyvers, 1997). While it preserves the core principles of REM—such as the storage of distinct episodic traces, feature-based representations, probabilistic storage, and a Bayesian-based decision process—it incorporates several significant elaborations. These extensions are specifically designed to handle the dynamic and multifaceted nature of context as it evolves over multiple learning episodes and to account for the modifications of memory traces that occur as a result of testing. This section provides a detailed description of the model's architecture, outlining its core components, the mechanisms governing context and memory updating, and the decision process.

#### **5.2.1. Core REM Principles**

At its heart, the model operates on the principles laid out by Shiffrin and Steyvers (1997). Memory consists of a collection of separate episodic traces, or images. Each trace is a vector of feature values, representing both the content of an experienced item (e.g., a picture) and the context in which it was encountered. When a new item is studied, a new episodic trace is created. This storage process is imperfect: the resulting trace is an incomplete and potentially error-prone copy of the original event's feature vector.

During a recognition test, the feature vector of the test probe is matched in parallel against all relevant traces in memory. The core of the decision process is the calculation of a likelihood ratio, λ, for each memory trace. This ratio quantifies the evidence provided by the match between the probe and a given trace:

λj​=P(matchj​∣d-image)P(matchj​∣s-image)​

where matchj​ represents the observed feature matches and mismatches for trace j. The numerator is the probability of observing that match if the trace is an "s-image" (a trace of the same item as the probe), and the denominator is the probability of observing that match if the trace is a "d-image" (a trace of a different item). The overall evidence for the probe being "old" is then calculated by averaging the likelihood ratios from all activated traces. This average, referred to as the odds, is compared to a decision criterion to yield an "old" or "new" judgment. Our extended model builds upon this foundation, specifying in greater detail the nature of the feature vectors and the processes that govern their creation and modification.

#### **5.2.2. Context Representation and Dynamics**

A central feature of the extended model is its sophisticated representation of context. Context is not a monolithic entity but is composed of distinct feature sets with different dynamic properties. For any given list, ℓ, the model assumes a total of 75 features, divided into three categories. This composition is designed to capture both the stable elements of the experimental setting and the more transient, fluctuating aspects of the temporal and mental context.

- **5.2.2.1. Changing Context (CC) Features:** The model includes 25 Changing Context (CC) features. These features represent the elements of context that are most likely to vary from one distinct episode to the next, such as the transition between different study lists. The model implements this by stipulating that each of the 25 CC features has a probability, δlist​=0.14, of changing its value between the end of one list and the beginning of the next. This mechanism allows the model to create distinct contextual signatures for each of the 10 study lists, providing a basis for list discrimination.
    
- **5.2.2.2. Unchanging Context (UC) Features:** In addition to the dynamic CC features, the model includes 25 Unchanging Context (UC) features. These represent the more stable aspects of the experimental environment that persist across the entire session, such as the physical location of the experiment, the computer interface, or the overarching task instructions. These features do not change between lists and thus provide a source of shared context across all episodes, contributing to the general feeling of familiarity for any item from the experiment.
    
- **5.2.2.3. Content (C) Features:** The model includes 25 Content (C) features. These features are directly tied to the perceptual and semantic information of the specific item being studied (in this case, a picture). They represent the unique identity of the stimulus itself, distinct from the context of its presentation.
    
- **5.2.2.4. Context Drift (**δdrift​**):** Within a single list, context is not assumed to be static. The model incorporates a mechanism of context drift to capture slow, random fluctuations in a participant's mental state over short periods. Between the end of a study phase and the beginning of the corresponding test phase, each context feature (both CC and UC) has a probability, δdrift​=0.14, of changing its value. This drift introduces a mismatch between the context encoded at study and the context present at test, providing a potential mechanism for forgetting and for serial position effects within a list.
    
- **5.2.2.5. Context Reinstatement (**δreinstate​**):** To account for the possibility that participants can mentally travel back to a previous context, the model includes a context reinstatement mechanism. After each test trial, each drifted context feature has a probability, δreinstate​=0.4, of reverting to its original value from the study phase of that list. This process is not perfect but allows the study context to be partially maintained across the test phase, counteracting the effects of context drift and output interference. A reinstatement probability of 0.4 ensures that after several trials, the context is highly likely to have returned to a state very similar to the original study context.
    

#### **5.2.3. Feature Encoding and Storage**

The encoding of features into a memory trace is governed by a set of probabilistic rules, consistent with the core REM framework.

- **5.2.3.1. Geometric Base Rates (**g**):** The values for all features (Content, CC, and UC) are drawn from a geometric distribution, which assumes that low feature values are common and high values are rare. This is a key principle in REM, as rare feature matches provide much stronger evidence for a match than common feature matches. The model assumes different base rates for content and context, reflecting the idea that contextual information may have different statistical properties than item content. The geometric base rate for content (word) features is set to gword​=0.4, while the base rate for context features is set to gcontext​=0.3.
    
- **5.2.3.2. Probability of Storage (**U∗**):** Not all features of an event are successfully stored in memory. The probability of storing a given feature, U∗, is a critical parameter. The model assumes that this probability can vary depending on the novelty of the event. Specifically, storage is assumed to be enhanced for the very first list and for the first item in each test phase, reflecting potential primacy or attention-orienting effects.
    
    - For studied items in the **first list**, U∗=0.08.
        
    - For studied items in all **subsequent lists** (2-10), U∗=0.04.
        
    - For the **first test item** in the first list, storage upon testing (see 5.2.4) is U∗=0.1.
        
    - For the first test item in subsequent lists (2-10), storage upon testing is U∗=0.08.
        
        For all other test items, the storage probability is the standard 0.04.
        
- **5.2.3.3. Copying Parameter (**c**):** When a feature is selected for storage, it is not always copied perfectly. The copying parameter, c=0.8, represents the probability that the feature's value is stored correctly. With probability 1−c, an error occurs, and a new value is drawn from the base rate geometric distribution instead.
    

#### **5.2.4. Memory Update During Testing (Restorage)**

A significant extension in this model is the assumption that the act of taking a recognition test is itself an encoding event that can modify the contents of memory. This "restorage" process allows the model to account for the effects of testing on subsequent memory performance, a key focus of the final test phase.

- **Strengthening of Content Features:** When a test item is correctly identified as "old" (a hit), the model assumes the system retrieves the most strongly matching memory trace (the one with the highest likelihood ratio). The content features of this retrieved trace are then compared with the content features of the test probe. Any content features in the trace that were previously blank (value of 0) or mismatched now have a probability of being updated to match the probe's value. This strengthening process applies **only to content features**, reflecting the idea that testing reinforces the memory for the item itself, but may not necessarily update its original context.
    
- **New Trace Formation for Foils:** When a test item is presented and judged (regardless of whether it's called "old" or "new"), a new memory trace is formed for it. If the item was a foil, this is its first encoding event. The content and context features of the foil probe undergo the same probabilistic storage process as a studied item (governed by U∗ and c). This mechanism allows the model to account for why foils presented during the initial tests can later be falsely recognized as "old" in the final test—because a real memory trace exists for them.
    
- **Perfect Storage of Targets:** The model includes a special assumption for the restorage of items tested as targets. To capture the potent effect of successful retrieval and recognition, the model assumes that the content features of a target item are restudied with perfect fidelity during the test. This is implemented by setting the storage probability to 1 and the copy probability to 1 for the content features of the target trace being updated. In contrast, a foil that is tested has its content features stored with the standard probability of U∗=0.04 and c=0.8. This creates a strong representational difference between items that were successfully recognized as targets and items that were merely exposed as foils.
    

#### **5.2.5. Decision Process**

The recognition decision is modeled as a sequential, two-step process that first uses context to constrain memory search and then uses content to make the final judgment.

- **5.2.5.1. Context Filtering:** The first step involves using the context of the test probe (its CC and UC features) to activate a relevant subset of traces from memory. The probe's context vector is compared to the context vectors of all traces in memory, and a likelihood ratio is computed for each based only on context features. Any trace whose context-based likelihood ratio exceeds a high threshold, τ=100, is considered part of the "activated set." This process acts as a filter, preferentially selecting traces that were encoded in a similar context and excluding those from dissimilar contexts (e.g., from distant lists).
    
- **5.2.5.2. Content Feature Evaluation:** In the second step, the content features of the test probe are compared only against the traces within the activated set. A new set of likelihood ratios is calculated based solely on the match of content features. The average of these content-based likelihoods yields the final odds. This value is then compared against a decision criterion, Θj​. A key aspect of the model is that this criterion is not fixed. It is assumed to decrease linearly across the test positions, j, within a list, from an initial value of Θ1​=1.0 to a final value of Θ20​=0.6. This declining criterion reflects a strategic shift by the participant, who may become more lenient in their judgments as they progress through a test list, perhaps due to fatigue or an awareness of accumulating interference.
    

#### **5.2.6. Model Parameters for the Final Test**

The final, cumulative test phase is modeled with a few specific parameter adjustments to reflect the different nature of this terminal test.

- **Criterion:** The decision criterion for the final test is set to a strict, constant value of Θ=1.0. This represents the idea that for a final, comprehensive test, participants adopt a more conservative strategy, requiring stronger evidence for an "old" judgment.
    
- **Context Composition:** Retrieval in the final test is assumed to be guided by a context probe that heavily emphasizes stable, long-term contextual information. The probe is composed of 90% unchanging context (UC) features and only 10% changing context (CC) features. This reflects the hypothesis that when retrieving from across the entire experiment, participants rely more on what was constant throughout, rather than the list-specific details that have since faded.
    
- **Final Test Context Drift:** A very slow context drift is assumed to occur during the final test. The probability that any feature of the changing context portion of the probe changes its value from one trial to the next is very small, pf​=0.02. This captures minor fluctuations in context during the lengthy final test block.
    

A summary table of the key model parameters is provided in the Appendix. The combination of these architectural features and dynamic processes provides a comprehensive framework for simulating the complex interplay of context and item memory in the recognition paradigms of this dissertation.

### **5.3. Modeling Experiment 1**

Experiment 1, which utilized exclusively novel foils in its initial test phases, provides the foundational dataset for evaluating the extended REM model. The goal of this modeling effort is to demonstrate that the architecture described in Section 5.2 can, with a single set of parameters, qualitatively and quantitatively account for the rich set of behavioral patterns observed. This includes performance across lists (between-list effects), performance as a function of study and test position (within-list effects), and, critically, performance on the final cumulative test, where the mnemonic history of each item plays a crucial role. This section details the application of the model to the data from Experiment 1, comparing simulation results to the empirical findings.

#### **5.3.1. Application of the Model to Experiment 1 Data**

The model was implemented in a computer simulation that precisely mirrored the structure of Experiment 1. The simulation proceeded through 10 study-test lists, followed by a final test. For each simulated participant, a unique set of feature vectors was generated for all stimuli (studied items and novel foils) according to the specified geometric base rates.

- **Simulating the Initial Study-Test Phase:** In each of the 10 simulated lists, 20 items were presented for study. The model encoded a memory trace for each, storing a subset of its content and context features according to the probabilities U∗ and c. The context vector for each study list was generated by applying the inter-list change probability, δlist​, to the CC features from the previous list. Following study, the context was made to drift with probability δdrift​. Then, a test phase of 20 trials (10 targets, 10 novel foils) was simulated. On each test trial, the decision process—context filtering followed by content evaluation against the linearly decreasing criterion Θj​—was executed. The model's "old" or "new" judgment was recorded. Critically, after each test judgment, the memory store was updated. For hits, the corresponding target trace was strengthened. For all test items (targets and foils), a new trace representing the test event itself was stored, capturing the fact that all test items, including foils, were experienced events.
    
- **Simulating the Final Test Phase:** After all 10 lists were completed, the simulation entered the final test phase. All 200 items that had been studied across the 10 lists, along with 200 new novel foils, were presented for recognition. The model's memory now contained a complex history: original study traces for all 200 targets, strengthened traces for the 100 targets that had been tested and hit, and new traces for the 100 targets and 100 foils from the initial test phases. The final test decision was made using the specific final test parameters: a strict criterion (Θ=1.0), a context probe weighted towards unchanging features, and a slow context drift (pf​=0.02). The simulation was run for a large number of iterations to generate stable, averaged results for comparison with the human data. The model was also run separately for the three final test ordering conditions (Random, Forward, Backward) by manipulating the sequence of test probes presented to the model.
    

#### **5.3.2. Simulation of Initial Test Performance**

A primary goal of the model is to capture the patterns of performance observed during the initial 10 study-test lists. These patterns reveal how memory adapts over the course of the experiment, reflecting the influence of proactive interference, context dynamics, and strategic criterion shifts.

- **Between-List Effects:** The empirical data from Experiment 1 (Chapter 3, Figure 3) show a characteristic pattern across the 10 lists. The hit rate for targets tends to decrease over the first few lists before stabilizing, while the correct rejection rate for novel foils remains relatively high and stable throughout. This divergence suggests that as the experiment progresses, it becomes harder to distinguish targets from the general background of experimental items, a classic sign of proactive interference.
    
    The model provides a principled account for this pattern. The **declining hit rate** is a direct consequence of accumulating interference and context dynamics. As more traces from prior lists are added to memory, the pool of potential competitors for any given probe increases. During context filtering, while the current list's context is most similar to the probe, traces from recent past lists may share enough contextual features (especially the unchanging UC features and any CC features that did not change) to pass the activation threshold, τ. This increases the "noise" in the activated set, slightly lowering the average likelihood ratio for targets and thus reducing the hit rate. The effect is most pronounced in the early lists as the memory store rapidly fills, and it stabilizes as the rate of new trace addition becomes a smaller proportion of the total traces stored.
    
    Conversely, the model predicts a **stable, high correct rejection rate**, matching the empirical data. This is because the foils in Experiment 1 are always novel. A novel foil has no pre-existing trace in memory. Therefore, when it is presented as a probe, it will not match any stored trace well. The likelihood ratios generated will be uniformly low, and the resulting odds will almost always fall below the decision criterion, leading to a correct "new" response. The accumulation of traces from prior lists does not significantly increase the false alarm rate to novel foils because none of those traces correspond to the foil probe. This demonstrates a key success of the model: it can simultaneously account for declining hit rates (due to interference among old items) and stable correct rejection rates (due to the lack of any representation for new items) without needing to postulate ad-hoc mechanisms. Figure 5.1 (top panels) shows the model's predictions for between-list performance, which closely track the observed data.
    
    _(Self-correction: The original prompt provided images of data and predictions. I will describe these comparisons as if the figures are present in the text, as per standard dissertation format.)_
    
- **Within-List Effects: Test Position:** The data for test position (output interference) show that performance for both targets and foils changes as a function of their position within the 20-trial test block. Typically, hit rates may show a slight decline, while false alarm rates (if any) might rise, but a more prominent feature is often a U-shaped accuracy curve or changes reflecting criterion shifts.
    
    The extended REM model captures these effects primarily through two mechanisms: the **linearly decreasing decision criterion (**Θj​**)** and **memory updating during test**. The decreasing criterion (Θ1​=1.0 to Θ20​=0.6) directly predicts that participants become more liberal in their responding as the test list progresses. This leads to an increase in the hit rate and also an increase in the false alarm rate across test positions. The model's prediction of an increasing hit rate across the test list (Figure 5.1, middle right panel) matches the trend seen in the empirical data (Chapter 3, Figure [e.g., Serial Position Data plot]). This is a powerful demonstration of how a simple, strategic parameter can account for a complex behavioral pattern.
    
    Simultaneously, the memory updating process contributes to output interference. Each test item, whether target or foil, creates a new trace. This means that an item tested late in the list faces more interference from traces created by items tested earlier _within the same list_. These fresh, highly similar-context traces are very likely to be activated by the context filter, adding noise to the decision and potentially suppressing the likelihood ratio for the actual target. The combination of these two factors—a strategic criterion shift and trace-based output interference—allows the model to provide a nuanced account of performance changes across the test phase.
    
- **Within-List Effects: Study Position:** The effect of an item's original study position on its final testability is a classic finding, often yielding a U-shaped serial position curve with both primacy and recency effects. The data from Experiment 1 (Chapter 3, Figure 1) show clear evidence of both.
    
    The model accounts for the **recency effect** through the mechanism of **context drift (**δdrift​**)**. Items studied at the end of the list are encoded in a context that is most similar to the context at the start of the test phase. Because context drift between study and test is a probabilistic process, the temporal proximity of late-list items to the test phase means their encoded context vectors have undergone less cumulative drift and are therefore a better match for the test probe's context. This leads to higher context-based likelihood ratios, a cleaner activated set, stronger overall odds, and a higher hit rate.
    
    The model accounts for the **primacy effect** through a different mechanism: **enhanced storage probability (**U∗**) for early items**. As specified in the architecture (Section 5.2.3.2), the model assumes that items in the very first list receive a higher base storage probability (U∗=0.08). This reflects the idea that items encountered when the context is novel receive more attentional resources, leading to the formation of stronger, more complete memory traces. These more robust traces are more likely to survive interference and produce a strong match signal at test, resulting in a higher hit rate for early-list items. The combination of enhanced storage for early items and contextual advantage for late items allows the model to reproduce the full U-shaped study position curve observed in the data (Figure 5.1, bottom left panel).
    

#### **5.3.3. Simulation of Final Test Performance**

The final cumulative test is the most rigorous challenge for the model, as it must account for the long-term retention of items based on their specific and varied histories from the initial phase. The key empirical finding from Experiment 1's final test is that memory performance depends heavily on an item's history: items that were studied and successfully tested as targets are remembered best, followed by items that were only studied, while items that were only ever seen as foils are remembered least well (but still above chance).

The model's architecture provides a direct explanation for this hierarchy, rooted in the assumptions about memory updating during the initial tests.

- **Performance for `Studied-and-Tested` Items (Targets):** These items benefit from two distinct encoding events. They have their original study trace, and they have the powerful trace strengthening that occurs upon a successful hit during the initial test. As described in 5.2.4, this strengthening process is assumed to be "perfect" for content features, creating a highly robust and accurate memory trace. When this item is probed in the final test, this high-fidelity trace generates a very strong likelihood ratio, leading to the highest hit rate, which aligns perfectly with the empirical results.
    
- **Performance for `Studied-Only` Items:** These items were part of the study list but were not selected for the initial test. Therefore, they only have their original study trace. This trace is subject to the standard probabilistic encoding (U∗=0.04, c=0.8) and was never strengthened. While this trace is sufficient for good recognition, it is less robust than the strengthened trace of a tested target. Consequently, the model predicts, and the data show, that these items have a lower hit rate in the final test compared to the `Studied-and-Tested` items.
    
- **Performance for `Tested-Only` Items (Foils):** These items were novel foils during the initial tests. They were never studied. However, the model's memory updating mechanism posits that a new trace is formed for every test item. Therefore, a memory trace _does_ exist for these initial-phase foils. This trace, however, was formed under standard probabilistic encoding conditions during the test and was never subjected to the "perfect" strengthening reserved for targets. This means it is a relatively weak trace, comparable in strength to a `Studied-Only` item's trace. When probed in the final test, this weak trace can generate enough evidence to be called "old" some of the time, leading to a false alarm rate that is significantly above the false alarm rate for brand new foils in the final test. This ability to predict above-chance false alarms for old foils is a critical success of the model, demonstrating the theoretical importance of assuming that testing is an encoding event.
    
- **Effects of Final Test Order (Random, Forward, Backward):** The model also accounts for how the order of retrieval in the final test can influence performance. In the **Forward** condition, the test probes are presented in an order that roughly mimics the original study order. This allows for a partial reinstatement of the evolving context across lists. As the test progresses from items of List 1 to items of List 2, the slow final test context drift (pf​=0.02) can be "entrained" by the sequence of retrieved traces, creating a test context that is a better match for the items being probed. This contextual match facilitates retrieval and leads to higher overall performance, as seen in the data. In the **Backward** condition, the context is systematically mismatched, potentially creating maximal interference and leading to the poorest performance. In the **Random** condition, any potential for contextual reinstatement is disrupted. The test context fluctuates randomly, providing no systematic advantage or disadvantage. The model predicts performance in the Random condition to be intermediate between Forward and Backward, consistent with the data. These predictions highlight the model's ability to use its dynamic context mechanisms to explain the subtle but important effects of retrieval organization. Figure 5.2 shows the model's predictions for the final test phase across item types and test orders, demonstrating a strong qualitative match with the experimental results.
    

#### **5.3.4. Comparison of Model Predictions with E1 Empirical Data**

Overall, the extended REM model provides an excellent and comprehensive account of the data from Experiment 1. Its strengths are numerous:

1. **Unified Explanation:** With a single set of architectural assumptions and parameters, the model accounts for between-list proactive interference, within-list serial position effects (both study and test), the differential long-term fate of items based on their testing history, and the effects of retrieval order in the final test.
    
2. **Principled Mechanisms:** The model's predictions are not the result of ad-hoc rules but emerge naturally from its core theoretical components: context drift, context change between lists, trace accumulation, probabilistic storage, memory updating during test, and a two-stage decision process.
    
3. **Qualitative and Quantitative Fit:** As shown in the comparisons between simulation results and empirical data (Figures 5.1 and 5.2), the model does not just predict the existence of these effects but also captures their direction and shape with remarkable fidelity. The U-shaped study position curve, the diverging hit/correct-rejection rates across lists, and the precise ordering of performance for different item types in the final test are all well-reproduced.
    

There are, of course, minor points of divergence. The model may, in some cases, slightly over- or under-predict the magnitude of an effect. For example, the predicted primacy effect might be slightly sharper than the one observed, or the performance in the final test's backward condition might be slightly lower than the data. These small discrepancies are not considered failures of the model's core structure. Rather, they likely reflect the necessary simplifications made in the model (e.g., the exact form of the criterion shift, the specific probabilities chosen for storage or drift) or sources of noise in the human data not captured by the deterministic components of the simulation. The crucial takeaway is that the fundamental architecture of the model is sound and possesses the necessary components to explain the complex cognitive processes at play.

#### **5.3.5. Parameter Fitting and Sensitivity Analyses**

The parameters used in the simulation were selected based on a combination of prior work with the REM model and a focused, but not exhaustive, search process aimed at achieving a good qualitative fit to the key patterns in the data. The goal was not to find the absolute best-fitting parameters through a complex optimization algorithm, but rather to demonstrate that a plausible set of parameters exists for which the model's inherent structure generates the correct behavioral phenomena.

Brief sensitivity analyses confirm the importance of the key architectural extensions. For example, removing the **memory updating** mechanism during test eliminates the model's ability to predict the different outcomes for `Studied-and-Tested`, `Studied-Only`, and `Tested-Only` items in the final test. Without it, all studied items would be equivalent, and all foils would have no representation. Similarly, setting the **context drift** parameter (δdrift​) to zero abolishes the recency effect in the study position curve. Removing the **linearly decreasing criterion** (Θj​) flattens the performance curve across test positions. The success of the model is therefore critically dependent on these new components, underscoring their theoretical significance. The model is not merely fitting the data; its explanatory power comes from a structure that embodies specific psychological hypotheses about how context operates and how memory changes with experience. The successful application of this model to Experiment 1 provides a strong foundation for the even greater challenge posed by the mixed-foil design of Experiment 2.