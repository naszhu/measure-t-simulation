
## daily note
-  today [allDay:: true]

1. Read Malmberg (2008), 
Got the question of why what REM is doing why it can produce *list strength effect* but traditional global matching SDT models cant.  Figured out that 1. REM give information quality difference on top of pure magnitude difference. 2. related to the last one in a way, REM uses nonlinear transformation that makes the quality difference to not linearly associate with magnitude difference. Therefore, list strength increase, the familiarity of other items doesn't necessarily increase. 

Then I somehow went into thinking about what is [[dot product]]. 

About REM
- **Information Quality vs. Magnitude:** Traditional models treat "strength" as "louder" (magnitude). REM treats "strength" as "clearer" (quality/precision).
    
- **Non-Linear Transformation:** REM uses a Likelihood Ratio (Odds) calculation. This transforms linear "matches" into logarithmic "evidence." Crucially, this means that adding features (strengthening an item) doesn't just add to the noise; it actively suppresses noise because mismatches are exponentially punished.
- 

2. Sent an email to Rich in solving the primacy and recency issue. 
---
[[Question]] Why [[REM]] likelihood ratio is dot product? (weighted dot product) 

[[concentration of measure]]

---
Based on the theoretical framework presented in the paper and general mathematical principles, here is the breakdown of your questions regarding rotation, weighted dot products, and probability distributions.

### 1. Are they the same system, just one rotates and one doesn't?

**Yes, your intuition is profound and correct.** Broadly speaking, both are global matching systems that sum evidence. The difference lies in **how they define the vector space** and **how they move within it**.

- **Traditional (MINERVA 2 / TODAM):**
    
    - **The Move:** When you learn, you add vectors. $\mathbf{V}_{new} = \mathbf{V}_{old} + \mathbf{V}_{study}$.
        
    - **The Geometry:** If $\mathbf{V}_{old}$ and $\mathbf{V}_{study}$ are the same (repetition), the vector gets longer (Magnitude increase). It does **not** change direction (Rotation).
        
    - **The Result:** It gets "louder" but not "sharper." The angle to foils stays the same.
        
- **REM:**
    
    - **The Move:** When you learn, you **replace** zeros with specific numbers.
        
    - **The Geometry:** You start at the origin (0,0,0 - representing "nothing known"). As you learn features (e.g., [0, 5, 0] $\rightarrow$ [2, 5, 9]), the vector moves _away_ from the origin into a specific quadrant of the hypercube.
        
    - **The Result:** This **IS** a rotation. You are rotating from "undefined/neutral" to "specific." By rotating into a specific corner of the space, you increase the angle between the target and the foils (which are in other corners). This is **Differentiation**1111.
        

### 2. Can you make a "Weighted Dot Product" model? (The "Simpler REM")

**Yes, and you have essentially reinvented Modern Information Retrieval.**

If you take the log of the REM equation, you get a weighted dot product.

- **REM (Multiplication):** $Score = \prod (\text{Likelihood Ratio})$
    
- **Log-REM (Addition):** $\log(Score) = \sum \log(\text{Likelihood Ratio})$
    

This formula $\sum \log(\text{Likelihood Ratio})$ **IS** a weighted dot product where the "weight" of a dimension is determined by its rarity.

**Has this been done?**

- **In Psychology:** This is effectively what **TF-IDF** (Term Frequency-Inverse Document Frequency) models do in semantic memory. Rare words (high information) get high weights; common words get low weights.
    
- **In Neural Networks:** **Attention Mechanisms** (like in Transformers/GPT) are dynamic weighted dot products. They calculate a "query-key" dot product and weight it by relevance.
    
- **Is it novel?**
    
    - Mathematically, it is not "new" because Log-REM is mathematically equivalent to it.
        
    - _However_, framing it explicitly as a **"Weighted Dot Product Model"** without the complex Bayesian derivations would be a valid, simpler modeling pattern. It would likely fit the data just as well as REM because it performs the same mathematical operations, just described geometrically (weights) rather than probabilistically (odds).
        

### 3. Why Geometric? Why not Gaussian?

The paper states REM uses **geometrically distributed feature values**2. There is a fundamental reason why **Gaussian (Normal)** distributions would fail to produce the sharp differentiation REM needs.

#### A. Discrete vs. Continuous (The "Lock and Key" Problem)

- **Gaussian:** Assumes **Continuous** space. $1.0$ is close to $1.1$.
    
    - If you use Gaussian, a "mismatch" is just a distance. Small errors are forgiven.
        
- **Geometric:** Assumes **Discrete** (Integer) space.
    
    - REM features are "symbolic." Feature value "5" might mean "Red," and value "20" might mean "Blue."
        
    - In a discrete space, "Red" is not "close" to "Blue." They are just **different**.
        
    - The Geometric distribution models integers ($1, 2, 3... \infty$). It allows the model to say: "This feature is a mismatch. It is the WRONG symbol."
        

#### B. The "Punishment" Factor (Tail Behavior)

- **Gaussian:** The "bell curve" is flat at the top.
    
    - If a foil is "sort of close" (within 1 standard deviation), a Gaussian model gives it a high probability score. It is "forgiving."
        
- **Geometric:** The probability drops immediately and exponentially ($P(k) = (1-g)^{k-1}g$).
    
    - **Rare Values:** The geometric distribution has a "long tail." High integers are very rare.
        
    - **Differentiation:** If a target has a rare feature (e.g., value 50) and the foil has a value of 2, the **Likelihood Ratio** is massive because $P(50)$ is tiny.
        
    - **Why Gaussian fails here:** To get the same "punishment," you would need a Gaussian with a tiny variance. But if the variance is tiny, you can never get a match unless the values are identical. The Geometric distribution allows for a "Symbolic Match" logic that fits human recognition data (which is often binary: I recognize it or I don't) better than continuous metric distance.
        

Summary:

You absolutely could build a Gaussian version (people have, in models like ALCOVE), but Gaussian models struggle with the List-Strength Effect because they are too "smooth." They don't punish mismatches hard enough to create the differentiation (the "Rotation") that REM achieves via its discrete, "unforgiving" geometric probability ratios.